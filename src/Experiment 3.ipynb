{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import seaborn as sns\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from settings import *\n",
    "from glove import *\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from textblob import TextBlob\n",
    "from tweets_ops import *\n",
    "from url_helpers import analyze_url\n",
    "from matching import *\n",
    "from textstat.textstat import textstat\n",
    "import urllib, bs4\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "\n",
    "figures = conf['aux_dir']+'figure-eight/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = pd.read_csv(cache_dir+'article_details_v6.tsv', sep='\\t')\n",
    "df7 = pd.read_csv(cache_dir+'article_details_v7.tsv', sep='\\t').fillna(0).drop('Alexa Rank', axis=1)\n",
    "df7['Author Signature'] = df6['authors'].apply(lambda x: len(eval(x)) != 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crispr = pd.read_csv(cache_dir+'crispr.tsv', sep='\\t').merge(df7, left_on='article', right_on='url')\n",
    "dfD = pd.read_csv(figures+'crispr-Dimitra.csv').rename(columns={'how_do_you_rate_the_scientific_quality_of_this_article': 'expert1'}).sort_values(by='article')\n",
    "dfJ = pd.read_csv(figures+'crispr-Jose.csv').rename(columns={'how_do_you_rate_the_scientific_quality_of_this_article': 'expert2'}).sort_values(by='article').drop('article', axis=1)\n",
    "df_exp_crispr = pd.concat([dfD,dfJ], axis=1)[['article', 'expert1', 'expert2']]\n",
    "df_exp_crispr.article = df_exp_crispr.article.apply(lambda x: x.replace('https://', 'http://'))\n",
    "df_exp_crispr['expert'] = (df_exp_crispr['expert1'] + df_exp_crispr['expert2'])/2\n",
    "df_crispr = df_crispr.merge(df_exp_crispr[['article', 'expert']])\n",
    "\n",
    "df_atc = pd.read_csv(cache_dir+'alc_tob_caf.tsv', sep='\\t').merge(df7, left_on='article', right_on='url')\n",
    "dfAn = pd.read_csv(figures+'tobacco-Andreu.csv').rename(columns={'how_do_you_rate_the_scientific_quality_of_this_article': 'expert1'}).sort_values(by='article')\n",
    "dfAi = pd.read_csv(figures+'tobacco-Aina.csv').rename(columns={'how_do_you_rate_the_scientific_quality_of_this_article': 'expert2'}).sort_values(by='article').drop('article', axis=1)\n",
    "df_exp_atc = pd.concat([dfAn,dfAi], axis=1)[['article', 'expert1', 'expert2']]\n",
    "df_exp_atc.article = df_exp_atc.article.apply(lambda x: x.replace('https://', 'http://'))\n",
    "df_exp_atc['expert'] = (df_exp_atc['expert1'] + df_exp_atc['expert2'])/2\n",
    "df_atc = df_atc.merge(df_exp_atc[['article', 'expert']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_crispr = np.array(df_crispr.drop(['article', 'paper', 'title', 'url', 'expert'], axis=1).values, dtype=np.float32)\n",
    "y_crispr = df_crispr['expert'].values\n",
    "y_crispr = y_crispr.round().astype(int)\n",
    "\n",
    "X_atc = np.array(df_atc.drop(['article','paper1', 'title1', 'paper2', 'title2', 'paper3', 'title3', 'title', 'url', 'expert'], axis=1).values, dtype=np.float32)\n",
    "y_atc = df_atc['expert'].values\n",
    "y_atc = y_atc.round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7['url'] = df7['url'].apply(lambda x: analyze_url(x)[0])\n",
    "df7 = df7.merge(pd.read_csv(cache_dir+'acsh.tsv', sep='\\t'), left_on='url', right_on='outlet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8=df7\n",
    "#df8 = df7[df7['url'].isin(['nytimes.com', 'theatlantic.com', 'dailymail.co.uk'])] #theatlantic.com, dailymail.co.uk, nytimes.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(df8.drop(['outlet', 'url', 'rate'], axis=1).values, dtype=np.float32)\n",
    "y_train = df8['rate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Article Word Count\n",
       "1           Readability\n",
       "2        #Users Friends\n",
       "3            #Followers\n",
       "4         #Users Tweets\n",
       "5                   STS\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_est = 250\n",
    "m_dep = 200\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=n_est, max_depth=m_dep, n_jobs=-1, random_state=42)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "imp = classifier.feature_importances_\n",
    "\n",
    "d = {c:r for c, r in zip(df7.drop(['outlet', 'url', 'rate'], axis=1).columns, imp)}\n",
    "pd.DataFrame(sorted(d.items(), key=lambda kv: kv[1], reverse=True))[0][:6]\n",
    "\n",
    "\n",
    "# classifier.fit(X_train, y_train)\n",
    "# print(n_est, m_dep, classifier.score(X_atc, y_atc))\n",
    "# df_atc['scilens'] = classifier.predict(X_atc)\n",
    "\n",
    "#classifier.fit(X_train, y_train)\n",
    "#print(n_est, m_dep, classifier.score(X_crispr, y_crispr))    \n",
    "# df_crispr['scilens'] = classifier.predict(X_crispr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_atc[['article', 'scilens']].to_csv(cache_dir+'scilens_atc.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
