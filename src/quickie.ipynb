{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from settings import *\n",
    "from utils import human_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(first_level_graph_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tweets:', human_format(len(df['tweet'].unique())))\n",
    "print('URLs:', human_format(len(df['out_url'])))\n",
    "print('Tweets without URL:', human_format(len(df['out_url'][df['out_url'].isin(['http://TweetWithoutURL.org'])])))\n",
    "print('HTTP Errors:', human_format(len(df['out_url'][df['out_url'].isin(['http://HTTPError.org'])])))\n",
    "print('Timeout Errors:', human_format(len(df['out_url'][df['out_url'].isin(['http://TimeoutError.org'])])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep only year\n",
    "df['date'] = df['date'].apply(lambda s : datetime.strptime(s, '%d.%m.%y %H:%M').year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df.copy()\n",
    "df1['out_url'] = df1['out_url'].apply(lambda u: u if u in ['http://TweetWithoutURL.org', 'http://HTTPError.org', 'http://TimeoutError.org'] else 'http://WorkingURL.org')\n",
    "df1[['tweet', 'date','out_url']].pivot_table(index='date', columns='out_url',aggfunc='count').T.reset_index(level=0, drop=True).T.fillna(1).plot(logy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning\n",
    "df = df[~df['out_url'].isin(['http://TweetWithoutURL.org', 'http://HTTPError.org', 'http://TimeoutError.org'])]\n",
    "df['netloc'] = df.apply(lambda r: re.sub(r'^(http://)?(www\\.)?', r'', '{0.netloc}'.format(urlsplit(r['out_url']))), axis=1)\n",
    "#mostly photos and reposts\n",
    "df = df[~df['netloc'].isin(['twitter.com', 'facebook.com', 'google.com'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Unique URLs:', human_format(len(df['out_url'].unique())))\n",
    "print('Unique network locations:', human_format(len(df['netloc'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = pd.read_csv(institutionsFile, sep='\\t')\n",
    "inst['URL'] = inst['URL'].apply(lambda u: re.sub(r'^(www[0-9]?\\.)|(web\\.)', r'', u))\n",
    "def find_inst(netloc, inst):\n",
    "    for i in inst:\n",
    "        if i==netloc or i in netloc:\n",
    "            return i\n",
    "    return netloc\n",
    "\n",
    "df['netloc'] = df['netloc'].apply(lambda r: find_inst(r, inst['URL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst = df.merge(inst, left_on='netloc', right_on='URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst.groupby('Institution').size().sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most popular Institutions')\n",
    "inst.groupby('Institution').mean()['popularity'].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst.groupby('Institution').mean().plot.scatter(x='Score', y='popularity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = inst.groupby('Institution').mean()[['popularity', 'World Rank', 'National Rank', 'Alumni Employment', 'Publications', 'Influence', 'Citations', 'Broad Impact', 'Patents', 'Score']].corr()\n",
    "#sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values)\n",
    "corr.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.read_csv(countriesFile)\n",
    "countries = inst[['user_country', 'Location']].merge(countries, left_on='user_country', right_on='Code')[['Name', 'Location']]\n",
    "countries.loc[countries['Location'] == 'USA'] = 'United States'\n",
    "countries['Name'] = countries['Name'].map(lambda n: n+'_user')\n",
    "countries['Location'] = countries['Location'].map(lambda n: n+'_inst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "B = nx.Graph()\n",
    "B.add_edges_from([(row['Name'], row['Location']) for _, row in countries.iterrows()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "X, Y = bipartite.sets(B)\n",
    "pos = dict()\n",
    "pos.update( (n, (1, i)) for i, n in enumerate(X) ) # put nodes from X at x=1\n",
    "pos.update( (n, (2, i*4)) for i, n in enumerate(Y) ) # put nodes from Y at x=2\n",
    "nx.draw(B, pos=pos, with_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repositories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos = pd.read_csv(repositoriesFile)\n",
    "repos['URL'] = repos['URL'].apply(lambda u: re.sub(r'^http://(www\\.)?', r'', u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repo(netloc, repos):\n",
    "    for i in repos:\n",
    "        if i==netloc or i in netloc:\n",
    "            return i\n",
    "    return netloc\n",
    "\n",
    "df['netloc'] = df['netloc'].apply(lambda r: find_repo(r, repos['URL']))\n",
    "repos = df.merge(repos, left_on='netloc', right_on='URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repos['Name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['tweet'].isin(inst['tweet'].tolist()+repos['tweet'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(second_level_urls_file, 'w') as f:\n",
    "    #f.write('URL\\tnetloc\\n')\n",
    "    for u in df['out_url'].unique():\n",
    "        f.write(u+'\\t'+re.sub(r'^(http://)?(www\\.)?', r'', '{0.netloc}'.format(urlsplit(u)))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
