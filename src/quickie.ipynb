{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import seaborn as sns\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "from settings import *\n",
    "from glove import *\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from textblob import TextBlob\n",
    "from tweets_ops import *\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alc_tob_caf use-case\n",
    "# articles = pd.read_csv(cache_dir+'article_details_v5.tsv', sep='\\t')\n",
    "\n",
    "# articles = articles[articles.topics.apply(lambda x: (max(eval(x)[0]) != 0.062500) & (max(eval(x)[0]) == eval(x)[0][11]))]\n",
    "# articles = articles[articles.entities.apply(lambda x: any(key in x for key in ['alcohol', 'tobacco', 'caffeine']))]\n",
    "\n",
    "# articles.to_csv(cache_dir+'alc_tob_caf.tsv', sep='\\t', index=None)\n",
    "\n",
    "# aggregate_tweet_details(cache_dir+'diffusion_graph/pruned_graph_v3.tsv', cache_dir+'tweet_details_v2.tsv', cache_dir+'alc_tob_caf.tsv', cache_dir+'alc_tob_caf.tsv')\n",
    "\n",
    "# articles = pd.read_csv(cache_dir+'alc_tob_caf.tsv', sep='\\t')\n",
    "# articles = articles[articles.replies.apply(lambda x: x != '[]')][['url', 'title']]\n",
    "\n",
    "# df1 = pd.read_csv(cache_dir+'similarity_model/train_pairs_v1.tsv', sep='\\t')\n",
    "# df1 = df1[df1['related']]\n",
    "# df2 = pd.read_csv(cache_dir+'similarity_model/test_pairs_v3.tsv', sep='\\t')\n",
    "# df2 = df2[df2['related']]\n",
    "# df = pd.concat([df1, df2]).drop('related', axis=1)\n",
    "\n",
    "# articles.merge(df, left_on='url', right_on='article').drop('url', axis=1).groupby(['article', 'title'])['paper'].apply(lambda x: '\\n'.join(x)).reset_index().to_csv(cache_dir+'alc_tob_caf.tsv', sep='\\t')\n",
    "\n",
    "#crispr use-case\n",
    "# articles = pd.read_csv(cache_dir+'article_details_v5.tsv', sep='\\t')\n",
    "\n",
    "# articles = articles[articles.entities.apply(lambda x: any(key in x for key in ['CRISPR']))]\n",
    "# articles = articles[articles['url'].apply(lambda x: not x.startswith('http://www.coach-boursier.com'))]\n",
    "\n",
    "# articles.to_csv(cache_dir+'crispr.tsv', sep='\\t', index=None)\n",
    "\n",
    "# aggregate_tweet_details(cache_dir+'diffusion_graph/pruned_graph_v3.tsv', cache_dir+'tweet_details_v2.tsv', cache_dir+'crispr.tsv', cache_dir+'crispr.tsv')\n",
    "\n",
    "# articles = pd.read_csv(cache_dir+'crispr.tsv', sep='\\t')\n",
    "\n",
    "# df1 = pd.read_csv(cache_dir+'similarity_model/train_pairs_v1.tsv', sep='\\t')\n",
    "# df1 = df1[df1['related']]\n",
    "# df2 = pd.read_csv(cache_dir+'similarity_model/test_pairs_v3.tsv', sep='\\t')\n",
    "# df2 = df2[df2['related']]\n",
    "# df = pd.concat([df1, df2]).drop('related', axis=1)\n",
    "\n",
    "# articles.merge(df, left_on='url', right_on='article').drop('url', axis=1).groupby(['article', 'title'])['paper'].apply(lambda x: '\\n'.join(x)).reset_index().to_csv(cache_dir+'crispr.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #one source articles\n",
    "# df = pd.read_csv(cache_dir+'similarity_model/test_pairs_v3.tsv', sep='\\t')\n",
    "\n",
    "# df = pd.DataFrame(df.groupby(['article'])['related'].apply(list).reset_index()['related'])\n",
    "\n",
    "# one_source = pd.read_csv(cache_dir+'similarity_model/train_pairs_v1.tsv', sep='\\t').shape[0]\n",
    "# articles_num = df.shape[0] + one_source\n",
    "\n",
    "# one_source += df[df['related'].apply(lambda x: sum(x)==1)].shape[0]\n",
    "\n",
    "# one_source/articles_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snopes = pd.read_csv('/home/psmeros/Dropbox/scilens/snopes-u.csv')\n",
    "# articles = pd.read_csv(cache_dir+'article_details_v5.tsv', sep='\\t')\n",
    "\n",
    "# def get_snopes_articles(title, snopes_list, snopes_list_vec):\n",
    "#     for sno, vec in zip(snopes_list, snopes_list_vec):\n",
    "#         title_vec = sent2vec(title)\n",
    "#         sim = cos_sim(vec, title_vec)\n",
    "#         if sim > 0.8:\n",
    "#             print('title:', title)\n",
    "#             print('snopes:', sno)\n",
    "#             print(sim)\n",
    "#     return\n",
    "\n",
    "\n",
    "# articles.apply(lambda x: get_snopes_articles(x['title'], snopes['claim'].tolist(), snopes['claim'].apply(sent2vec).tolist()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stance_data_dir = conf['aux_dir'] + 'small_files/stance/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@tedcruz And, #HandOverTheServer she wiped cle...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hillary is our best choice if we truly want to...</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@TheView I think our country is ready for a fe...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I just gave an unhealthy amount of my hard-ear...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@PortiaABoulger Thank you for adding me to you...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hillary can not win. Here's hoping the Dems of...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Respect FOR the law and respect BY the law Yes...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I don't want to be appointed to an Ambassador ...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#StopHillary2016 @HillaryClinton if there was ...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@HillaryClinton End lawless #ClintonFoundation...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Use your brain, keep Hillary out of the White ...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>@HillaryClinton Hillary pandering with her log...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>@readyforHRC @HillaryClinton #HillaryClinton, ...</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@CiaraAntaya cuz you know I'm such a feminist</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2 million bogus followers on Twitter @HillaryC...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@lindasuhler : My name is Rebecca and my grand...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Where's the campaign store is the real questio...</td>\n",
       "      <td>FAVOR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>It's a miracle, suddenly #Democrats don't mind...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@smileitsalicia @greekgummybear2 now i can liv...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hillary doesn't want to put anyone in prison a...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>The only way I support Hillary was if Elizabet...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>@HomeOfUncleSam @ScotsFyre @RWNutjob1 @SA_Hart...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Because Communist Breadlines are not my thing!...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>@HillaryClinton bad wife, bad role model for w...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Everything Hillary touches ends up being a sca...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Yes HRC subject 2 dbl standard Smh Come on @bi...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>#Hillary to stop for #pizza today to garner th...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>I want America to great again #WhyImNotVotingF...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>March 8, 2016 Ohio is holding our Primaries! T...</td>\n",
       "      <td>NONE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>@RIGHTZONE @WethePeoplePets Let's hope the VOT...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5798</th>\n",
       "      <td>@HomunculusLoikm @SawyerAndrew @BryanBroome1 @...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5799</th>\n",
       "      <td>@HomunculusLoikm @SawyerAndrew @BryanBroome1 @...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5800</th>\n",
       "      <td>@MI_IMMACULATA @GLORYtoGOD_143 @BenignoVito Ve...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5801</th>\n",
       "      <td>Your body belongs to you. It doesn't belong to...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5802</th>\n",
       "      <td>Never during a pregnancy does a pregnant perso...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5803</th>\n",
       "      <td>People go nuts on the #Onepercent, yet their f...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5804</th>\n",
       "      <td>RT @LisetteHasHope: Yes I will be at @theRally...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5805</th>\n",
       "      <td>everyone should have the right to make their o...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5806</th>\n",
       "      <td>#PeoplesDebate Juvenile hysterics from that wo...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5807</th>\n",
       "      <td>#PeoplesDebate Better every child born out of ...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5808</th>\n",
       "      <td>@toby_dorena I am in favor of women's rights a...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5809</th>\n",
       "      <td>There's a gray area when it comes to abortion....</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5810</th>\n",
       "      <td>You were alive for up to 9 months before you w...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5811</th>\n",
       "      <td>@mrgeology They abort millions of their possib...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5812</th>\n",
       "      <td>@oXGodLessXo @Canuckle_head @Atheist_Roo @atxb...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5813</th>\n",
       "      <td>@notmuchelse And the unborn ARE human lives co...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5814</th>\n",
       "      <td>@toby_dorena Any pregnancy can turn deadly at ...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5815</th>\n",
       "      <td>#Love does not delight in #evil, but #rejoices...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5816</th>\n",
       "      <td>The same people that demand gov stay out of th...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5817</th>\n",
       "      <td>@ghhshirley Charged with Manslaughter After An...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5818</th>\n",
       "      <td>@ghhshirley Abortion Workers Charged with Mans...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5819</th>\n",
       "      <td>@ghhshirley @WordfactoryQ @LeahNTorres Abortio...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5820</th>\n",
       "      <td>@NARAL @TIME At the same time, this decision I...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5821</th>\n",
       "      <td>If your agonist abortion get a vasectomy  #SemST</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>When you bring up the 50 million black babies ...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823</th>\n",
       "      <td>There's a law protecting unborn eagles, but no...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5824</th>\n",
       "      <td>I am 1 in 3... I have had an abortion #Abortio...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5825</th>\n",
       "      <td>How dare you say my sexual preference is a cho...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5826</th>\n",
       "      <td>Equal rights for those 'born that way', no rig...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5827</th>\n",
       "      <td>#POTUS seals his legacy w/ 1/2 doz wins. The #...</td>\n",
       "      <td>AGAINST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5828 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet   Stance\n",
       "0     @tedcruz And, #HandOverTheServer she wiped cle...  AGAINST\n",
       "1     Hillary is our best choice if we truly want to...    FAVOR\n",
       "2     @TheView I think our country is ready for a fe...  AGAINST\n",
       "3     I just gave an unhealthy amount of my hard-ear...  AGAINST\n",
       "4     @PortiaABoulger Thank you for adding me to you...     NONE\n",
       "5     Hillary can not win. Here's hoping the Dems of...  AGAINST\n",
       "6     Respect FOR the law and respect BY the law Yes...     NONE\n",
       "7     I don't want to be appointed to an Ambassador ...     NONE\n",
       "8     #StopHillary2016 @HillaryClinton if there was ...  AGAINST\n",
       "9     @HillaryClinton End lawless #ClintonFoundation...  AGAINST\n",
       "10    Use your brain, keep Hillary out of the White ...  AGAINST\n",
       "11    @HillaryClinton Hillary pandering with her log...  AGAINST\n",
       "12    @readyforHRC @HillaryClinton #HillaryClinton, ...    FAVOR\n",
       "13        @CiaraAntaya cuz you know I'm such a feminist     NONE\n",
       "14    2 million bogus followers on Twitter @HillaryC...  AGAINST\n",
       "15    @lindasuhler : My name is Rebecca and my grand...     NONE\n",
       "16    Where's the campaign store is the real questio...    FAVOR\n",
       "17    It's a miracle, suddenly #Democrats don't mind...  AGAINST\n",
       "18    @smileitsalicia @greekgummybear2 now i can liv...     NONE\n",
       "19    Hillary doesn't want to put anyone in prison a...  AGAINST\n",
       "20    The only way I support Hillary was if Elizabet...  AGAINST\n",
       "21    @HomeOfUncleSam @ScotsFyre @RWNutjob1 @SA_Hart...  AGAINST\n",
       "22    Because Communist Breadlines are not my thing!...  AGAINST\n",
       "23    @HillaryClinton bad wife, bad role model for w...  AGAINST\n",
       "24    Everything Hillary touches ends up being a sca...  AGAINST\n",
       "25    Yes HRC subject 2 dbl standard Smh Come on @bi...  AGAINST\n",
       "26    #Hillary to stop for #pizza today to garner th...  AGAINST\n",
       "27    I want America to great again #WhyImNotVotingF...  AGAINST\n",
       "28    March 8, 2016 Ohio is holding our Primaries! T...     NONE\n",
       "29    @RIGHTZONE @WethePeoplePets Let's hope the VOT...  AGAINST\n",
       "...                                                 ...      ...\n",
       "5798  @HomunculusLoikm @SawyerAndrew @BryanBroome1 @...  AGAINST\n",
       "5799  @HomunculusLoikm @SawyerAndrew @BryanBroome1 @...  AGAINST\n",
       "5800  @MI_IMMACULATA @GLORYtoGOD_143 @BenignoVito Ve...  AGAINST\n",
       "5801  Your body belongs to you. It doesn't belong to...  AGAINST\n",
       "5802  Never during a pregnancy does a pregnant perso...  AGAINST\n",
       "5803  People go nuts on the #Onepercent, yet their f...  AGAINST\n",
       "5804  RT @LisetteHasHope: Yes I will be at @theRally...  AGAINST\n",
       "5805  everyone should have the right to make their o...  AGAINST\n",
       "5806  #PeoplesDebate Juvenile hysterics from that wo...  AGAINST\n",
       "5807  #PeoplesDebate Better every child born out of ...  AGAINST\n",
       "5808  @toby_dorena I am in favor of women's rights a...  AGAINST\n",
       "5809  There's a gray area when it comes to abortion....  AGAINST\n",
       "5810  You were alive for up to 9 months before you w...  AGAINST\n",
       "5811  @mrgeology They abort millions of their possib...  AGAINST\n",
       "5812  @oXGodLessXo @Canuckle_head @Atheist_Roo @atxb...  AGAINST\n",
       "5813  @notmuchelse And the unborn ARE human lives co...  AGAINST\n",
       "5814  @toby_dorena Any pregnancy can turn deadly at ...  AGAINST\n",
       "5815  #Love does not delight in #evil, but #rejoices...  AGAINST\n",
       "5816  The same people that demand gov stay out of th...  AGAINST\n",
       "5817  @ghhshirley Charged with Manslaughter After An...  AGAINST\n",
       "5818  @ghhshirley Abortion Workers Charged with Mans...  AGAINST\n",
       "5819  @ghhshirley @WordfactoryQ @LeahNTorres Abortio...  AGAINST\n",
       "5820  @NARAL @TIME At the same time, this decision I...  AGAINST\n",
       "5821   If your agonist abortion get a vasectomy  #SemST  AGAINST\n",
       "5822  When you bring up the 50 million black babies ...  AGAINST\n",
       "5823  There's a law protecting unborn eagles, but no...  AGAINST\n",
       "5824  I am 1 in 3... I have had an abortion #Abortio...  AGAINST\n",
       "5825  How dare you say my sexual preference is a cho...  AGAINST\n",
       "5826  Equal rights for those 'born that way', no rig...  AGAINST\n",
       "5827  #POTUS seals his legacy w/ 1/2 doz wins. The #...  AGAINST\n",
       "\n",
       "[5828 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv(stance_data_dir+'train.tsv', sep='\\t'),pd.read_csv(stance_data_dir+'train.tsv', sep='\\t')]).reset_index(drop=True)\n",
    "df = df[['Tweet', 'Stance']]\n",
    "#df['Stance'] = df['Stance'].apply(lambda x: )\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df['Tweet'].apply(lambda x: sid.polarity_scores(x)).apply(pd.Series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cache_dir+'../corpus/stance_classification.csv').rename(columns={'what_do_you_believe_is_the_repliers_stance_position_towards_the_tweet': 'stance', 'what_do_you_believe_is_the_repliers_stance_position_towards_the_tweet:confidence': 'confidence'})\n",
    "df = df[df.confidence>0.5][['full_text', 'reply', 'stance']]\n",
    "df = df[df['stance']!='nr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv(cache_dir+'tweet_details_v2.tsv', sep='\\t')\n",
    "df = tweets[['full_text']]\n",
    "df['reply'] = tweets['replies'].apply(lambda x: ' '.join(eval(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sim'] = df.apply(lambda x: cos_sim(sent2vec(x['full_text']), sent2vec(x['reply'])),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['reply'].apply(lambda x: len((re.sub(' +',' ',re.sub(r'[^a-zA-Z0-9 ]', '', x))).strip().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['negation'] = df['reply'].apply(lambda x: any(n in x for n in [' no ', ' not ', 'n\\'t ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_words = open(conf['aux_dir'] + 'small_files/opinion/positive-words.txt', encoding='utf-8', errors='ignore').read().splitlines()\n",
    "negative_words = open(conf['aux_dir'] + 'small_files/opinion/negative-words.txt', encoding='utf-8', errors='ignore').read().splitlines()\n",
    "df['positive'] = df['reply'].apply(lambda x: sum(n in x for n in positive_words))\n",
    "df['negative'] = df['reply'].apply(lambda x: sum(n in x for n in negative_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['length'] = df['reply'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['has_url'] = df['reply'].apply(lambda x: bool(re.search('http(s)?://', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['quest_mark'] = df['reply'].apply(lambda x: x.count('?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['excl_mark'] = df['reply'].apply(lambda x: x.count('!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reply_polarity'] =  df['reply'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df['reply_subjectivity'] =  df['reply'].apply(lambda x: TextBlob(x).sentiment.subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['stance'] = df['stance'].apply(lambda x: 'con-quest' if x in ['con', 'quest'] else x).apply(lambda x: 'nr-com-sup' if x in ['nr', 'com', 'sup']  else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(['full_text', 'reply', 'stance'], axis=1).values, dtype=np.float32)\n",
    "y = df['stance'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val = False\n",
    "n_est = 100\n",
    "m_dep = 10\n",
    "fold = 2\n",
    "if cross_val:\n",
    "    kf = KFold(n_splits=fold, shuffle=True)\n",
    "    score = 0.0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        X_train = (X_train - X_train.mean(axis=0)) / (X_train.std(axis=0)+1e-9)\n",
    "        X_test = (X_test - X_test.mean(axis=0)) / (X_test.std(axis=0)+1e-9)\n",
    "        \n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        classifier = RandomForestClassifier(n_estimators=n_est, max_depth=m_dep, n_jobs=-1)\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        score += classifier.score(X_test, y_test)\n",
    "        conf_mat = confusion_matrix(y_test, classifier.predict(X_test))\n",
    "        conf_mat = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "        print(conf_mat)\n",
    "    print('Score:', score/fold)\n",
    "else:\n",
    "    classifier = RandomForestClassifier(n_estimators=n_est, max_depth=m_dep, n_jobs=-1)\n",
    "    X = (X - X.mean(axis=0)) / (X.std(axis=0)+1e-9)\n",
    "    classifier.fit(X, y)\n",
    "    pickle.dump(classifier, open(cache_dir+'stance.pkl', 'wb'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df.drop(['full_text', 'reply'], axis=1).values, dtype=np.float32)\n",
    "X = (X - X.mean(axis=0)) / (X.std(axis=0)+1e-9)\n",
    "classifier = pickle.load(open(cache_dir+'stance.pkl', 'rb'))\n",
    "tweets['stance']=classifier.predict_proba(X)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['stance']= tweets.apply(lambda x: 0 if x.replies_num==0 else x.stance, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(cache_dir+'tweet_details_v3.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
