{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from spacy.en import English\n",
    "from spacy.symbols import nsubj, dobj, VERB\n",
    "global nlp\n",
    "nlp = English()\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext(conf = (SparkConf().setMaster('local').setAppName('quoteExtraction')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from settings import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Keyword Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sourcesKeywords = [nlp(x)[0].lemma_ for x in ['paper', 'report', 'study', 'analysis', 'research', 'survey', 'release']]\n",
    "peopleKeywords = [nlp(x)[0].lemma_ for x in ['expert', 'scientist']]\n",
    "actionsKeywords = [nlp(x)[0].lemma_ for x in ['prove', 'demonstrate', 'reveal', 'state', 'mention', 'report', 'say', 'show', 'announce', 'claim', 'suggest', 'argue', 'predict', 'believe', 'think']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search (on the vector space) for sentences containing the given keywords. (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doKeywordSearch=False\n",
    "if doKeywordSearch:\n",
    "    #Load gloVe Embeddings\n",
    "    from gloveEmbeddings import loadGloveEmbeddings, word2vec\n",
    "    loadGloveEmbeddings(gloveFile)\n",
    "\n",
    "    sourcesKeywordsVec = [word2vec(x) for x in sourcesKeywords]\n",
    "    peopleKeywordsVec = [word2vec(x) for x in peopleKeywords]\n",
    "    actionsKeywordsVec = [word2vec(x) for x in actionsKeywords]\n",
    "\n",
    "def keywordSearch(title, body):\n",
    "    subjectThreshold = 0.9\n",
    "    predicateThreshold = 0.9\n",
    "    \n",
    "    claims = []\n",
    "    for s in sent_tokenize(body):\n",
    "        subjectFound = predicateFound = False\n",
    "        claim = \"\"\n",
    "        for w in wordpunct_tokenize(s):\n",
    "\n",
    "            if predicateFound == True:\n",
    "                claim = s\n",
    "                claims.append(claim)\n",
    "                break\n",
    "\n",
    "            wVec = word2vec(w)\n",
    "\n",
    "            if subjectFound == False:\n",
    "                for sVec in sourcesKeywordsVec+peopleKeywordsVec:\n",
    "                    if sim(sVec, wVec) > subjectThreshold:\n",
    "                        subjectFound = True\n",
    "                        break\n",
    "\n",
    "            if subjectFound == True:\n",
    "                for pVec in actionsKeywordsVec:\n",
    "                    if sim(pVec, wVec) > predicateThreshold:\n",
    "                        predicateFound = True\n",
    "                        break\n",
    "    return claims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for quote patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Improves quotee's name\n",
    "def improveQuotee(quotee, quoteeType, allEntities):\n",
    "\n",
    "    if len(quotee.split()) == 1:\n",
    "        #case where quotee is referred to with his/her first or last name.    \n",
    "        for e in allEntities:\n",
    "            if quotee in e.text.split() and quoteeType in ['PERSON']:\n",
    "                return e.text, e.label_\n",
    "\n",
    "        #case where quotee is referred to with an acronym.\n",
    "        def createAcronym(phrase):\n",
    "            fullAcronym = ''\n",
    "            compactAcronym = ''\n",
    "            upperAccronym = ''\n",
    "\n",
    "            if len(phrase.split()) > 1:\n",
    "                for w in phrase.split():\n",
    "                    for l in w:\n",
    "                        if (l.isupper()):\n",
    "                            upperAccronym += l\n",
    "                    if w not in stopWords:\n",
    "                        compactAcronym += w[0]\n",
    "                    fullAcronym += w[0]\n",
    "\n",
    "            return fullAcronym.lower(), compactAcronym.lower(), upperAccronym.lower()\n",
    "\n",
    "        for e in allEntities:\n",
    "            if quotee.lower() in createAcronym(e.text)  and quoteeType in ['ORG']:\n",
    "                return e.text, e.label_\n",
    "\n",
    "    return quotee, quoteeType\n",
    "\n",
    "#Resolves the quotee of a quote.\n",
    "def resolveQuotee(quotee, sentenceEntities, allEntities):\n",
    "    global nlp\n",
    "    \n",
    "    #heuristic: if there is no named entity as quotee, assume it's the first entity of the sentence.\n",
    "    useHeuristic = False\n",
    "    firstEntity = None\n",
    "    if useHeuristic:\n",
    "        for e in sentenceEntities + allEntities:\n",
    "            if e.label_ in ['PERSON', 'ORG']:\n",
    "                firstEntity = (e.text, e.label_)\n",
    "    try:\n",
    "        c = next(nlp(quotee).noun_chunks)\n",
    "    except:    \n",
    "        return firstEntity or ('', 'unknown')\n",
    "\n",
    "    #case that quotee entity exists.\n",
    "    for e in sentenceEntities:\n",
    "        if c.text == e.text and e.label_ in ['PERSON', 'ORG']:\n",
    "            return (e.text, e.label_)\n",
    "    \n",
    "    #case that quotee entity doesn't exist.\n",
    "    if c.root.lemma_ in sourcesKeywords:\n",
    "        return firstEntity or ('study', 'unknown')\n",
    "    elif c.root.lemma_ in peopleKeywords:\n",
    "        return firstEntity or('expert', 'unknown')\n",
    "    else:\n",
    "        return (c.text, 'unknown')\n",
    "\n",
    "def dependencyGraphSearch(title, body):\n",
    "    global nlp\n",
    "    \n",
    "    allEntities = nlp(body).ents + nlp(title).ents\n",
    "    quotes = []\n",
    "\n",
    "    for s in sent_tokenize(body):\n",
    "        quoteFound = quoteeFound = False\n",
    "        quote = quotee = quoteeType = \"\"\n",
    "\n",
    "        doc = nlp(s)\n",
    "\n",
    "        #find all verbs of the sentence.\n",
    "        verbs = set()\n",
    "        for v in doc:\n",
    "            if v.head.pos == VERB:\n",
    "                verbs.add(v.head)\n",
    "\n",
    "        if not verbs:\n",
    "            continue\n",
    "\n",
    "        rootVerb = ([w for w in doc if w.head is w] or [None])[0]\n",
    "\n",
    "        #check first the root verb and then the others.\n",
    "        verbs = [rootVerb] + list(verbs)\n",
    "\n",
    "        for v in verbs:\n",
    "            if v.lemma_ in actionsKeywords:            \n",
    "\n",
    "                for np in doc.noun_chunks:\n",
    "                    if np.root.head == v:\n",
    "\n",
    "                        if(np.root.dep == nsubj):\n",
    "                            quotee = np.text\n",
    "                            quoteeFound = True\n",
    "\n",
    "                        if(np.root.dep == dobj): #TODO\n",
    "                            pass\n",
    "\n",
    "                        quoteFound = True\n",
    "\n",
    "                if quoteeFound:\n",
    "                    break\n",
    "\n",
    "        if quoteFound:\n",
    "                quote = s                    \n",
    "                quotee, quoteeType = resolveQuotee(quotee, doc.ents, allEntities)\n",
    "                quotee, quoteeType = improveQuotee(quotee, quoteeType, allEntities)                    \n",
    "\n",
    "                quotes.append({'quote': quote, 'quotee':quotee, 'quoteeType':quoteeType})\n",
    "                #print('quote: ', quote)\n",
    "                #print('by: ', quotee, '(', quoteeType, ')')\n",
    "                #print()\n",
    "                continue\n",
    "\n",
    "    return quotes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#quotesPerDocument:  7.77\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "topic_label\n",
       "Addiction                                         22\n",
       "Bacteria, Disease                                 24\n",
       "Boycott, Ban, Campaign, Action                     6\n",
       "Children                                           9\n",
       "Congress, Government                              13\n",
       "Consumer, Demand, Supply                          55\n",
       "Crop, Contamination                               23\n",
       "Diet, Healthy Eating                              17\n",
       "Farm, Agriculture, Farmer                         83\n",
       "Fresh Food, Real Food, Healthy Food               17\n",
       "GMO, Bioengineering                               21\n",
       "Health, Well-Being, Wellness                      44\n",
       "Hunger, Availability, Security, Food, Shortage    15\n",
       "Insecticide, Toxicity, Pesticide, Poison          12\n",
       "Junk Food, Unhealthy Food                         43\n",
       "Law, Label, Labeling, GMO                         30\n",
       "Nutritional, Nutrient                             19\n",
       "Obesity, Fat, Weight                              66\n",
       "Palatable, Taste                                  24\n",
       "Profit, Revenue, Earnings                         11\n",
       "Recall, Health Risk                               39\n",
       "Research, Advancement, Progress                   32\n",
       "Salt, Sugar, Fat, Recommendation                  68\n",
       "Sugar, Diabetes                                   59\n",
       "Warming, Climate Change                            7\n",
       "Water Shortage                                    18\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quoteExtraction(limitDocuments=10):\n",
    "    query = createQuery(limitDocuments, 'web')\n",
    "    documents = queryDB(query)\n",
    "        \n",
    "    #TODO:Spark\n",
    "    #rddd = SQLContext(sc).createDataFrame(documents[['title','body']]).rdd\n",
    "    #documents['quotes'] = rddd.map(lambda s: dependencyGraphSearch(s.title, s.body)).collect()\n",
    "    \n",
    "    documents['quotes'] = documents.apply(lambda d: dependencyGraphSearch(d['title'],d['body']), axis=1)\n",
    "    documents = documents.join(documents['quotes'].apply(pd.Series).stack().reset_index(level=1, drop=True).apply(pd.Series))\n",
    "    \n",
    "    print ('#quotesPerDocument: ',len(documents)/limitDocuments)\n",
    "    return documents\n",
    "\n",
    "documents = quoteExtraction(100)\n",
    "documents.to_pickle('quotes.pkl')\n",
    "documents[['topic_label', 'quotee']].groupby('topic_label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
