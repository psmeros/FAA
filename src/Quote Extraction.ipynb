{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from spacy.en import English\n",
    "from spacy.symbols import nsubj, dobj, VERB\n",
    "global nlp\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext(conf = (SparkConf().setMaster('local[*]').setAppName('quoteExtraction')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from settings import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Keyword Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createKeywords():\n",
    "    nlp = English()\n",
    "    sourcesKeywords = [nlp(x)[0].lemma_ for x in ['paper', 'report', 'study', 'analysis', 'research', 'survey', 'release']]\n",
    "    peopleKeywords = [nlp(x)[0].lemma_ for x in ['expert', 'scientist']]\n",
    "    actionsKeywords = [nlp(x)[0].lemma_ for x in ['prove', 'demonstrate', 'reveal', 'state', 'mention', 'report', 'say', 'show', 'announce', 'claim', 'suggest', 'argue', 'predict', 'believe', 'think']]\n",
    "    return sourcesKeywords, peopleKeywords, actionsKeywords\n",
    "sourcesKeywords, peopleKeywords, actionsKeywords = createKeywords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search (on the vector space) for sentences containing the given keywords. (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doKeywordSearch=False\n",
    "if doKeywordSearch:\n",
    "    #Load gloVe Embeddings\n",
    "    from gloveEmbeddings import loadGloveEmbeddings, word2vec\n",
    "    loadGloveEmbeddings(gloveFile)\n",
    "\n",
    "    sourcesKeywordsVec = [word2vec(x) for x in sourcesKeywords]\n",
    "    peopleKeywordsVec = [word2vec(x) for x in peopleKeywords]\n",
    "    actionsKeywordsVec = [word2vec(x) for x in actionsKeywords]\n",
    "\n",
    "def keywordSearch(title, body):\n",
    "    subjectThreshold = 0.9\n",
    "    predicateThreshold = 0.9\n",
    "    \n",
    "    claims = []\n",
    "    for s in sent_tokenize(body):\n",
    "        subjectFound = predicateFound = False\n",
    "        claim = \"\"\n",
    "        for w in wordpunct_tokenize(s):\n",
    "\n",
    "            if predicateFound == True:\n",
    "                claim = s\n",
    "                claims.append(claim)\n",
    "                break\n",
    "\n",
    "            wVec = word2vec(w)\n",
    "\n",
    "            if subjectFound == False:\n",
    "                for sVec in sourcesKeywordsVec+peopleKeywordsVec:\n",
    "                    if sim(sVec, wVec) > subjectThreshold:\n",
    "                        subjectFound = True\n",
    "                        break\n",
    "\n",
    "            if subjectFound == True:\n",
    "                for pVec in actionsKeywordsVec:\n",
    "                    if sim(pVec, wVec) > predicateThreshold:\n",
    "                        predicateFound = True\n",
    "                        break\n",
    "    return claims\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for quote patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Improves quotee's name\n",
    "def improveQuotee(quotee, quoteeType, allEntities):\n",
    "\n",
    "    if len(quotee.split()) == 1:\n",
    "        #case where quotee is referred to with his/her first or last name.    \n",
    "        for e in allEntities:\n",
    "            if quotee in e.text.split() and quoteeType in ['PERSON']:\n",
    "                return e.text, e.label_\n",
    "\n",
    "        #case where quotee is referred to with an acronym.\n",
    "        def createAcronym(phrase):\n",
    "            fullAcronym = ''\n",
    "            compactAcronym = ''\n",
    "            upperAccronym = ''\n",
    "\n",
    "            if len(phrase.split()) > 1:\n",
    "                for w in phrase.split():\n",
    "                    for l in w:\n",
    "                        if (l.isupper()):\n",
    "                            upperAccronym += l\n",
    "                    if w not in stopWords:\n",
    "                        compactAcronym += w[0]\n",
    "                    fullAcronym += w[0]\n",
    "\n",
    "            return fullAcronym.lower(), compactAcronym.lower(), upperAccronym.lower()\n",
    "\n",
    "        for e in allEntities:\n",
    "            if quotee.lower() in createAcronym(e.text)  and quoteeType in ['ORG']:\n",
    "                return e.text, e.label_\n",
    "\n",
    "    return quotee, quoteeType\n",
    "\n",
    "#Resolves the quotee of a quote.\n",
    "def resolveQuotee(quotee, sentenceEntities, allEntities):\n",
    "\n",
    "    #heuristic: if there is no named entity as quotee, assume it's the first entity of the sentence.\n",
    "    useHeuristic = False\n",
    "    firstEntity = None\n",
    "    if useHeuristic:\n",
    "        for e in sentenceEntities + allEntities:\n",
    "            if e.label_ in ['PERSON', 'ORG']:\n",
    "                firstEntity = (e.text, e.label_)\n",
    "    try:\n",
    "        c = next(nlp(quotee).noun_chunks)\n",
    "    except:    \n",
    "        return firstEntity or ('', 'unknown')\n",
    "\n",
    "    #case that quotee entity exists.\n",
    "    for e in sentenceEntities:\n",
    "        if c.text == e.text and e.label_ in ['PERSON', 'ORG']:\n",
    "            return (e.text, e.label_)\n",
    "    \n",
    "    #case that quotee entity doesn't exist.\n",
    "    if c.root.lemma_ in sourcesKeywords:\n",
    "        return firstEntity or ('study', 'unknown')\n",
    "    elif c.root.lemma_ in peopleKeywords:\n",
    "        return firstEntity or('expert', 'unknown')\n",
    "    else:\n",
    "        return (c.text, 'unknown')\n",
    "\n",
    "def dependencyGraphSearch(title, body):\n",
    "    \n",
    "    global nlp\n",
    "    try: nlp('')\n",
    "    except: nlp = English()\n",
    "    \n",
    "    allEntities = nlp(body).ents + nlp(title).ents\n",
    "    quotes = []\n",
    "\n",
    "    for s in sent_tokenize(body):\n",
    "        quoteFound = quoteeFound = False\n",
    "        quote = quotee = quoteeType = \"\"\n",
    "\n",
    "        doc = nlp(s)\n",
    "\n",
    "        #find all verbs of the sentence.\n",
    "        verbs = set()\n",
    "        for v in doc:\n",
    "            if v.head.pos == VERB:\n",
    "                verbs.add(v.head)\n",
    "\n",
    "        if not verbs:\n",
    "            continue\n",
    "\n",
    "        rootVerb = ([w for w in doc if w.head is w] or [None])[0]\n",
    "\n",
    "        #check first the root verb and then the others.\n",
    "        verbs = [rootVerb] + list(verbs)\n",
    "\n",
    "        for v in verbs:\n",
    "            if v.lemma_ in actionsKeywords:            \n",
    "\n",
    "                for np in doc.noun_chunks:\n",
    "                    if np.root.head == v:\n",
    "\n",
    "                        if(np.root.dep == nsubj):\n",
    "                            quotee = np.text\n",
    "                            quoteeFound = True\n",
    "\n",
    "                        if(np.root.dep == dobj): #TODO\n",
    "                            pass\n",
    "\n",
    "                        quoteFound = True\n",
    "\n",
    "                if quoteeFound:\n",
    "                    break\n",
    "\n",
    "        if quoteFound:\n",
    "                quote = s                    \n",
    "                quotee, quoteeType = resolveQuotee(quotee, doc.ents, allEntities)\n",
    "                quotee, quoteeType = improveQuotee(quotee, quoteeType, allEntities)                    \n",
    "\n",
    "                quotes.append({'quote': quote, 'quotee':quotee, 'quoteeType':quoteeType})\n",
    "                #print('quote: ', quote)\n",
    "                #print('by: ', quotee, '(', quoteeType, ')')\n",
    "                #print()\n",
    "                continue\n",
    "\n",
    "    return quotes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def quoteExtraction(limitDocuments=10):\n",
    "    useSpark = True\n",
    "\n",
    "    query = createQuery(limitDocuments, 'web')\n",
    "    documents = queryDB(query)        \n",
    "\n",
    "    if (useSpark):\n",
    "        rddd = SQLContext(sc).createDataFrame(documents[['title','body']]).rdd\n",
    "        documents['quotes'] = rddd.map(lambda s: dependencyGraphSearch(s.title, s.body)).collect()\n",
    "    else:\n",
    "        documents['quotes'] = documents.apply(lambda d: dependencyGraphSearch(d['title'],d['body']), axis=1)\n",
    "    \n",
    "    documents = documents[['topic_label']].join(documents['quotes'].apply(pd.Series).stack().reset_index(level=1, drop=True).apply(pd.Series))\n",
    "    \n",
    "    print ('#quotesPerDocument: ',len(documents)/limitDocuments)\n",
    "    return documents\n",
    "\n",
    "documents = quoteExtraction(100)\n",
    "documents.to_pickle('quotes.pkl')\n",
    "documents[['topic_label', 'quotee']].groupby('topic_label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
