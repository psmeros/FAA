{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from spacy.symbols import nsubj, dobj, VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from settings import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gloveEmbeddings import *\n",
    "loadGloveEmbeddings(gloveFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates Keyword Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keywordVectors():\n",
    "    sources = ['paper', 'report', 'study', 'analysis', 'research', 'survey']\n",
    "    people = ['expert', 'scientist']\n",
    "    subjects = sources + people\n",
    "    predicates = ['prove', 'demonstrate', 'reveal', 'state', 'mention', 'report', 'say', 'show', 'announce', 'claim', 'suggest', 'argue']\n",
    "    \n",
    "    subjectsVec = [word2vec(s) for s in subjects]\n",
    "    predicatesVec = [word2vec(p) for p in predicates]\n",
    "    \n",
    "    return subjects, predicates, subjectsVec, predicatesVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searches (on the vector space) for sentences containing the given subject and predicate keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keywordSearch(title, body, subjectVec, predicateVec):\n",
    "    subjectThreshold = 0.9\n",
    "    predicateThreshold = 0.9\n",
    "\n",
    "    claims = []\n",
    "    for s in sent_tokenize(body):\n",
    "        subjectFound = predicateFound = False\n",
    "        claim = \"\"\n",
    "        for w in wordpunct_tokenize(s):\n",
    "\n",
    "            if predicateFound == True:\n",
    "                claim = s\n",
    "                claims.append(claim)\n",
    "                break\n",
    "\n",
    "            wVec = word2vec(w)\n",
    "\n",
    "            if subjectFound == False:\n",
    "                for sVec in subjectVec:\n",
    "                    if abs(cosine_similarity(sVec.reshape(1, -1), wVec.reshape(1, -1))) > subjectThreshold:\n",
    "                        subjectFound = True\n",
    "                        break\n",
    "\n",
    "            if subjectFound == True:\n",
    "                for pVec in predicateVec:\n",
    "                    if abs(cosine_similarity(pVec.reshape(1, -1), wVec.reshape(1, -1))) > predicateThreshold:\n",
    "                        predicateFound = True\n",
    "                        break\n",
    "    return claims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim: Three studies show the benefits of healthier school meals.\n",
      "by: Three studies\n",
      "\n",
      "claim: From The Rudd Center, now at U. Conn: A press release announces publication of its new study in Childhood Obesity demonstrating that the rules have led to an increase in fruit consumption without increasing plate waste.\n",
      "by: A press release\n",
      "\n",
      "claim: From the Harvard School of Public Health: It also sends a press release to announce its study demonstrating that an increase in consumption of fruits and vegetables is a direct result of the new USDA standards, and that these also do not increase plate waste.\n",
      "by: \n",
      "\n",
      "claim: From the Union of Concerned Scientists: UCS announces a new position paper, “Lessons from the Lunchroom: Childhood Obesity, School Lunch, and the Way to a Healthier Future,” also documenting why school meals are so important to kids’ health.\n",
      "by: UCS\n",
      "\n",
      "claim: Postscript: Dana Woldow argues that the school food scene would be much easier if schools actually got enough money to pay for what they serve and for decent wages to school food service workers.\n",
      "by: Dana Woldow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def claimExtraction(limitDocuments=10):\n",
    "    query = createQuery(limitDocuments, 'web')\n",
    "    documents = queryDB(query)\n",
    "\n",
    "        \n",
    "    subjects, predicates, subjectsVec, predicatesVec = keywordVectors() #global vars\n",
    "\n",
    "    def checkPredicate(w):\n",
    "        for p in predicates:\n",
    "            if w is not None and w.lemma_ == nlp(p)[0].lemma_:\n",
    "                return True\n",
    "        return False\n",
    "                \n",
    "#             wVec = word2vec(str(rootVerb))\n",
    "#             for pVec in predicatesVec:\n",
    "#                 if abs(cosine_similarity(pVec.reshape(1, -1), wVec.reshape(1, -1))) > 0.75:\n",
    "#                     predicateFound = True\n",
    "#                     print (\"sentence:\",s)\n",
    "#                     break\n",
    "    \n",
    "   \n",
    "    \n",
    "    def dependencyGraphSearch(title, body):\n",
    "        \n",
    "        claims = []\n",
    "        for s in sent_tokenize(body):\n",
    "            claimFound = claimerFound = False\n",
    "            claim = \"\"\n",
    "            claimer = \"\"\n",
    "            \n",
    "            doc = nlp(s)\n",
    "            \n",
    "            #find all verbs of the sentence.\n",
    "            verbs = set()\n",
    "            for v in doc:\n",
    "                if v.head.pos == VERB:\n",
    "                    verbs.add(v.head)\n",
    "            \n",
    "            if not verbs:\n",
    "                continue\n",
    " \n",
    "            rootVerb = ([w for w in doc if w.head is w] or [None])[0]\n",
    "            \n",
    "            #check first the root verb and then the others.\n",
    "            verbs = [rootVerb] + list(verbs)\n",
    "\n",
    "\n",
    "#             if (predicateFound):\n",
    "#                 if s.startswith('From the Harvard School'):\n",
    "#                     print ('sentence:', s)\n",
    "#                     print ('root:',rootVerb)\n",
    "                  \n",
    "            for v in verbs:\n",
    "                if checkPredicate(v):            \n",
    "                    \n",
    "                    for np in doc.noun_chunks:\n",
    "                        if np.root.head == v:\n",
    "\n",
    "                            if(np.root.dep == nsubj):\n",
    "                                claimer = np.text\n",
    "                                claimerFound = True\n",
    "                            \n",
    "                            if(np.root.dep == dobj): #TODO\n",
    "                                pass\n",
    "                                \n",
    "                            claimFound = True\n",
    "                    \n",
    "                    if claimerFound:\n",
    "                        break\n",
    "                            \n",
    "                    \n",
    "                \n",
    "            if claimFound:\n",
    "                    claim = s\n",
    "                    claims.append(claim)\n",
    "                    print('claim:', claim)\n",
    "                    print('by:', claimer)\n",
    "                    print()\n",
    "                    continue\n",
    "\n",
    "        #return claims\n",
    "\n",
    "    \n",
    "    claims = documents.apply(lambda d: dependencyGraphSearch(d['title'],d['body']), axis=1)\n",
    "    return claims[0]\n",
    "\n",
    "claimExtraction(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
