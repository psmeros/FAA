{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "from spacy.symbols import nsubj, dobj, VERB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from settings import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gloveEmbeddings import *\n",
    "loadGloveEmbeddings(gloveFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates Keyword Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sourcesKeywords = [nlp(x)[0].lemma_ for x in ['paper', 'report', 'study', 'analysis', 'research', 'survey', 'release']]\n",
    "peopleKeywords = [nlp(x)[0].lemma_ for x in ['expert', 'scientist']]\n",
    "actionsKeywords = [nlp(x)[0].lemma_ for x in ['prove', 'demonstrate', 'reveal', 'state', 'mention', 'report', 'say', 'show', 'announce', 'claim', 'suggest', 'argue', 'predict']]\n",
    "\n",
    "sourcesKeywordsVec = [word2vec(x) for x in sourcesKeywords]\n",
    "peopleKeywordsVec = [word2vec(x) for x in peopleKeywords]\n",
    "actionsKeywordsVec = [word2vec(x) for x in actionsKeywords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searches (on the vector space) for sentences containing the given subject and predicate keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keywordSearch(title, body):\n",
    "    subjectThreshold = 0.9\n",
    "    predicateThreshold = 0.9\n",
    "\n",
    "    claims = []\n",
    "    for s in sent_tokenize(body):\n",
    "        subjectFound = predicateFound = False\n",
    "        claim = \"\"\n",
    "        for w in wordpunct_tokenize(s):\n",
    "\n",
    "            if predicateFound == True:\n",
    "                claim = s\n",
    "                claims.append(claim)\n",
    "                break\n",
    "\n",
    "            wVec = word2vec(w)\n",
    "\n",
    "            if subjectFound == False:\n",
    "                for sVec in sourcesKeywordsVec+peopleKeywordsVec:\n",
    "                    if abs(cosine_similarity(sVec.reshape(1, -1), wVec.reshape(1, -1))) > subjectThreshold:\n",
    "                        subjectFound = True\n",
    "                        break\n",
    "\n",
    "            if subjectFound == True:\n",
    "                for pVec in actionsKeywordsVec:\n",
    "                    if abs(cosine_similarity(pVec.reshape(1, -1), wVec.reshape(1, -1))) > predicateThreshold:\n",
    "                        predicateFound = True\n",
    "                        break\n",
    "    return claims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim:  Three studies show the benefits of healthier school meals.\n",
      "by:  The School Nutrition Association ( ORG )\n",
      "\n",
      "claim:  From The Rudd Center, now at U. Conn: A press release announces publication of its new study in Childhood Obesity demonstrating that the rules have led to an increase in fruit consumption without increasing plate waste.\n",
      "by:  The Rudd Center ( ORG )\n",
      "\n",
      "claim:  From the Harvard School of Public Health: It also sends a press release to announce its study demonstrating that an increase in consumption of fruits and vegetables is a direct result of the new USDA standards, and that these also do not increase plate waste.\n",
      "by:  the Harvard School of Public Health ( ORG )\n",
      "\n",
      "claim:  From the Union of Concerned Scientists: UCS announces a new position paper, “Lessons from the Lunchroom: Childhood Obesity, School Lunch, and the Way to a Healthier Future,” also documenting why school meals are so important to kids’ health.\n",
      "by:  the Union of Concerned Scientists ( ORG )\n",
      "\n",
      "claim:  Postscript: Dana Woldow argues that the school food scene would be much easier if schools actually got enough money to pay for what they serve and for decent wages to school food service workers.\n",
      "by:  Dana Woldow ( ORG )\n",
      "\n",
      "claim:  A fledgling coalition of pizza chains — including Domino’s, Papa John’s, Little Caesars, Godfather’s Pizza and Pizza Hut — argues that the government’s plan forces store owners to pay for in-store menu boards that most of their customers don’t see before ordering.\n",
      "by:  A fledgling coalition ( unknown )\n",
      "\n",
      "claim:  “A light bulb goes on when people hear about all the possible combinations for pizza,” said Lynn Liddle, a Domino’s executive and chair of the coalition.\n",
      "by:  Lynn Liddle ( PERSON )\n",
      "\n",
      "claim:  Many studies show that calorie intake is higher when people dine out.\n",
      "by:  Domino's Pizza ( ORG )\n",
      "\n",
      "claim:  The American Pizza Community, created in January, says the rules as written are unfair.\n",
      "by:  The American Pizza Community ( ORG )\n",
      "\n",
      "claim:  For starters, the FDA would require that a menu board display the calorie count for the entire pizza even though the average consumer eats only 2.1 slices, the group said.\n",
      "by:  the group ( unknown )\n",
      "\n",
      "claim:  But such ranges can be so wide for pizza that they’re useless, the pizza makers said.\n",
      "by:  the pizza makers ( unknown )\n",
      "\n",
      "claim:  This week, through a spokesman, the agency said it is continuing to work on the rule.\n",
      "by:  the agency ( unknown )\n",
      "\n",
      "claim:  Mary Lyne Carraway, owner of nearly five dozen Domino’s stores in the Washington area, said the proposal would require new menu boards at each location at $3,000 to $5,000 a pop — and she’d probably end up replacing them once a year.\n",
      "by:  Mary Lyne Carraway ( PERSON )\n",
      "\n",
      "claim:  But Carraway predicts the changes won’t have the intended effect on consumer eating habits.\n",
      "by:  Mary Lyne Carraway ( PERSON )\n",
      "\n",
      "claim:  “I don’t see pizza counts dropping, and I don’t see salads running out the door,” Carraway said.\n",
      "by:  Mary Lyne Carraway ( PERSON )\n",
      "\n",
      "claim:  Although research is inconclusive on whether calorie listings influence buying decisions, health advocates say two large studies focused on New York City show that they do.\n",
      "by:  health advocates ( unknown )\n",
      "\n",
      "claim:  Margo Wootan, a director at the Center for Science in the Public Interest, said the FDA’s proposal offers a reasoned approach to tackling the menu labeling issue.\n",
      "by:  Margo Wootan ( PERSON )\n",
      "\n",
      "claim:  There’s also good reason to list the calorie count for a food item in its entirety, instead of per serving, Wootan said.\n",
      "by:  Margo Wootan ( PERSON )\n",
      "\n",
      "claim:  None of the issues the pizza makers are raising are new or unique to their business, Wootan said.\n",
      "by:  Margo Wootan ( PERSON )\n",
      "\n",
      "claim:  “We heard the same types of arguments from the whole restaurant industry when they were opposing menu labeling in the early days,” Wootan said.\n",
      "by:  Margo Wootan ( PERSON )\n",
      "\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def claimExtraction(limitDocuments=10):\n",
    "    query = createQuery(limitDocuments, 'web')\n",
    "    documents = queryDB(query)\n",
    "\n",
    "    global cc \n",
    "    cc = 0\n",
    "    \n",
    "    def checkAction(w):\n",
    "        threshold = 0.9\n",
    "        \n",
    "        for a in actionsKeywords:\n",
    "            if w is not None and w.lemma_ == a:\n",
    "                return True\n",
    "        \n",
    "        wVec = word2vec(w.text)\n",
    "\n",
    "        for aVec in actionsKeywordsVec:\n",
    "            if abs(cosine_similarity(aVec.reshape(1, -1), wVec.reshape(1, -1))) > threshold:\n",
    "                print (\"new action\",w)\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def acronym(phrase):\n",
    "        fullAcronym = ''\n",
    "        compactAcronym = ''\n",
    "        upperAccronym = ''\n",
    "        \n",
    "        if len(phrase.split()) > 1:\n",
    "            for w in phrase.split():\n",
    "                for l in w:\n",
    "                    if (l.isupper()):\n",
    "                        upperAccronym += l\n",
    "                if w not in stopWords:\n",
    "                    compactAcronym += w[0]\n",
    "                fullAcronym += w[0]\n",
    "                \n",
    "        return fullAcronym.lower(), compactAcronym.lower(), upperAccronym.lower()\n",
    "            \n",
    "    \n",
    "    def improveClaimerEntity(claimer, claimerType, allEntities):\n",
    "\n",
    "        if len(claimer.split()) == 1:\n",
    "            #case where claimer is referred to with his/her first or last name.    \n",
    "            for e in allEntities:\n",
    "                if claimer in e.text.split() and claimerType in ['PERSON']:\n",
    "                    return e.text, e.label_\n",
    "            \n",
    "            #case where claimer is referred to with an acronym.\n",
    "            for e in allEntities:\n",
    "                if claimer.lower() in acronym(e.text)  and claimerType in ['ORG']:\n",
    "                    return e.text, e.label_\n",
    "        \n",
    "        return claimer, claimerType\n",
    "    \n",
    "    def resolveClaimer(claimer, sentenceEntities, allEntities):\n",
    "        \n",
    "        try:\n",
    "            c = next(nlp(claimer).noun_chunks)\n",
    "        except:\n",
    "            #heuristic: first entity of the sentence or of the whole text.\n",
    "            for e in sentenceEntities + allEntities:\n",
    "                if e.label_ in ['PERSON', 'ORG']:\n",
    "                    return e.text, e.label_         \n",
    "            return '', 'unknown'        \n",
    "        \n",
    "        for e in sentenceEntities:\n",
    "            if c.text == e.text and e.label_ in ['PERSON', 'ORG']:\n",
    "                return c.text, e.label_\n",
    "                   \n",
    "        for w in sourcesKeywords:\n",
    "            if c.root.lemma_ == nlp(w)[0].lemma_:\n",
    "                #heuristic: first entity of the sentence or of the whole text.\n",
    "                for e in sentenceEntities + allEntities:\n",
    "                    if e.label_ in ['PERSON', 'ORG']:\n",
    "                        return e.text, e.label_\n",
    "                return 'study', 'unknown'\n",
    "\n",
    "        for w in peopleKeywords:\n",
    "            if c.root.lemma_ == nlp(w)[0].lemma_:\n",
    "                #heuristic: first entity of the sentence or of the whole text.\n",
    "                for e in sentenceEntities + allEntities:\n",
    "                    if e.label_ in ['ORG']:\n",
    "                        return e.text, e.label_\n",
    "                return 'expert', 'unknown'\n",
    "\n",
    "        return c.text, 'unknown'   \n",
    "    \n",
    "    def dependencyGraphSearch(title, body):\n",
    "        \n",
    "        \n",
    "        allEntities = nlp(body).ents + nlp(title).ents\n",
    "        claims = []\n",
    "        \n",
    "        for s in sent_tokenize(body):\n",
    "            claimFound = claimerFound = False\n",
    "            claim = claimer = claimerType = \"\"\n",
    "            \n",
    "            doc = nlp(s)\n",
    "            \n",
    "            #find all verbs of the sentence.\n",
    "            verbs = set()\n",
    "            for v in doc:\n",
    "                if v.head.pos == VERB:\n",
    "                    verbs.add(v.head)\n",
    "            \n",
    "            if not verbs:\n",
    "                continue\n",
    " \n",
    "            rootVerb = ([w for w in doc if w.head is w] or [None])[0]\n",
    "            \n",
    "            #check first the root verb and then the others.\n",
    "            verbs = [rootVerb] + list(verbs)\n",
    "            \n",
    "            for v in verbs:\n",
    "                if checkAction(v):            \n",
    "                    \n",
    "                    for np in doc.noun_chunks:\n",
    "                        if np.root.head == v:\n",
    "\n",
    "                            if(np.root.dep == nsubj):\n",
    "                                claimer = np.text\n",
    "                                claimerFound = True\n",
    "                                \n",
    "                            if(np.root.dep == dobj): #TODO\n",
    "                                pass\n",
    "                                \n",
    "                            claimFound = True\n",
    "                    \n",
    "                    if claimerFound:\n",
    "                        break\n",
    "                            \n",
    "                    \n",
    "                \n",
    "            if claimFound:\n",
    "                    claim = s\n",
    "                    global cc \n",
    "                    cc += 1\n",
    "                    claims.append(claim)\n",
    "                    claimer, claimerType = resolveClaimer(claimer, doc.ents, allEntities)\n",
    "                    claimer, claimerType = improveClaimerEntity(claimer, claimerType, allEntities)\n",
    "                    print('claim: ', claim)\n",
    "                    print('by: ', claimer, '(', claimerType, ')')\n",
    "                    print()\n",
    "                    continue\n",
    "\n",
    "        return claims\n",
    "\n",
    "    \n",
    "    claims = documents.apply(lambda d: dependencyGraphSearch(d['title'],d['body']), axis=1)\n",
    "    \n",
    "    print (cc)\n",
    "    return len(claims[0])\n",
    "\n",
    "claimExtraction(2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
