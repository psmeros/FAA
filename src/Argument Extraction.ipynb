{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from spacy.symbols import nsubj, dobj, VERB\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "sc = SparkContext(appName='quoteExtraction', master='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from settings import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loads GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gloveEmbeddings import *\n",
    "loadGloveEmbeddings(gloveFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates Keyword Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcesKeywords = [nlp(x)[0].lemma_ for x in ['paper', 'report', 'study', 'analysis', 'research', 'survey', 'release']]\n",
    "peopleKeywords = [nlp(x)[0].lemma_ for x in ['expert', 'scientist']]\n",
    "actionsKeywords = [nlp(x)[0].lemma_ for x in ['prove', 'demonstrate', 'reveal', 'state', 'mention', 'report', 'say', 'show', 'announce', 'claim', 'suggest', 'argue', 'predict', 'believe', 'think']]\n",
    "\n",
    "sourcesKeywordsVec = [word2vec(x) for x in sourcesKeywords]\n",
    "peopleKeywordsVec = [word2vec(x) for x in peopleKeywords]\n",
    "actionsKeywordsVec = [word2vec(x) for x in actionsKeywords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searches (on the vector space) for sentences containing the given subject and predicate keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def keywordSearch(title, body):\n",
    "    subjectThreshold = 0.9\n",
    "    predicateThreshold = 0.9\n",
    "\n",
    "    claims = []\n",
    "    for s in sent_tokenize(body):\n",
    "        subjectFound = predicateFound = False\n",
    "        claim = \"\"\n",
    "        for w in wordpunct_tokenize(s):\n",
    "\n",
    "            if predicateFound == True:\n",
    "                claim = s\n",
    "                claims.append(claim)\n",
    "                break\n",
    "\n",
    "            wVec = word2vec(w)\n",
    "\n",
    "            if subjectFound == False:\n",
    "                for sVec in sourcesKeywordsVec+peopleKeywordsVec:\n",
    "                    if abs(cosine_similarity(sVec.reshape(1, -1), wVec.reshape(1, -1))) > subjectThreshold:\n",
    "                        subjectFound = True\n",
    "                        break\n",
    "\n",
    "            if subjectFound == True:\n",
    "                for pVec in actionsKeywordsVec:\n",
    "                    if abs(cosine_similarity(pVec.reshape(1, -1), wVec.reshape(1, -1))) > predicateThreshold:\n",
    "                        predicateFound = True\n",
    "                        break\n",
    "    return claims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd created\n"
     ]
    }
   ],
   "source": [
    "def quoteExtraction(limitDocuments=10):\n",
    "    query = createQuery(limitDocuments, 'web')\n",
    "    documents = queryDB(query)\n",
    "\n",
    "    global cc \n",
    "    cc = 0\n",
    "    \n",
    "    def checkAction(w):\n",
    "        enableEmbeddings = False\n",
    "        threshold = 0.9\n",
    "        \n",
    "        for a in actionsKeywords:\n",
    "            if w is not None and w.lemma_ == a:\n",
    "                return True\n",
    "        \n",
    "        if (enableEmbeddings):\n",
    "            wVec = word2vec(w.text)\n",
    "\n",
    "            for aVec in actionsKeywordsVec:\n",
    "                if abs(cosine_similarity(aVec.reshape(1, -1), wVec.reshape(1, -1))) > threshold:\n",
    "                    print (\"new action\",w)\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def acronym(phrase):\n",
    "        fullAcronym = ''\n",
    "        compactAcronym = ''\n",
    "        upperAccronym = ''\n",
    "        \n",
    "        if len(phrase.split()) > 1:\n",
    "            for w in phrase.split():\n",
    "                for l in w:\n",
    "                    if (l.isupper()):\n",
    "                        upperAccronym += l\n",
    "                if w not in stopWords:\n",
    "                    compactAcronym += w[0]\n",
    "                fullAcronym += w[0]\n",
    "                \n",
    "        return fullAcronym.lower(), compactAcronym.lower(), upperAccronym.lower()\n",
    "            \n",
    "    \n",
    "    def improveQuoteeEntity(quotee, quoteeType, allEntities):\n",
    "\n",
    "        if len(quotee.split()) == 1:\n",
    "            #case where quotee is referred to with his/her first or last name.    \n",
    "            for e in allEntities:\n",
    "                if quotee in e.text.split() and quoteeType in ['PERSON']:\n",
    "                    return e.text, e.label_\n",
    "            \n",
    "            #case where quotee is referred to with an acronym.\n",
    "            for e in allEntities:\n",
    "                if quotee.lower() in acronym(e.text)  and quoteeType in ['ORG']:\n",
    "                    return e.text, e.label_\n",
    "        \n",
    "        return quotee, quoteeType\n",
    "    \n",
    "    def resolveQuotee(quotee, sentenceEntities, allEntities):\n",
    "        \n",
    "        nlp = spacy.load('en')\n",
    "        try:\n",
    "            c = next(nlp(quotee).noun_chunks)\n",
    "        except:\n",
    "            #heuristic: first entity of the sentence or of the whole text.\n",
    "            for e in sentenceEntities + allEntities:\n",
    "                if e.label_ in ['PERSON', 'ORG']:\n",
    "                    return e.text, e.label_         \n",
    "            return '', 'unknown'        \n",
    "        \n",
    "        for e in sentenceEntities:\n",
    "            if c.text == e.text and e.label_ in ['PERSON', 'ORG']:\n",
    "                return c.text, e.label_\n",
    "                   \n",
    "        for w in sourcesKeywords:\n",
    "            if c.root.lemma_ == w:\n",
    "                #heuristic: first entity of the sentence.\n",
    "                for e in sentenceEntities:\n",
    "                    if e.label_ in ['PERSON', 'ORG']:\n",
    "                        return e.text, e.label_\n",
    "                return 'study', 'unknown'\n",
    "\n",
    "        for w in peopleKeywords:\n",
    "            if c.root.lemma_ == w:\n",
    "                #heuristic: first entity of the sentence.\n",
    "                for e in sentenceEntities:\n",
    "                    if e.label_ in ['PERSON', 'ORG']:\n",
    "                        return e.text, e.label_\n",
    "                return 'expert', 'unknown'\n",
    "\n",
    "        return c.text, 'unknown'   \n",
    "    \n",
    "    def dependencyGraphSearch(title, body):\n",
    "        \n",
    "        nlp = spacy.load('en')\n",
    "        allEntities = nlp(body).ents + nlp(title).ents\n",
    "        quotes = []\n",
    "        \n",
    "        for s in sent_tokenize(body):\n",
    "            quoteFound = quoteeFound = False\n",
    "            quote = quotee = quoteeType = \"\"\n",
    "            \n",
    "            doc = nlp(s)\n",
    "            \n",
    "            #find all verbs of the sentence.\n",
    "            verbs = set()\n",
    "            for v in doc:\n",
    "                if v.head.pos == VERB:\n",
    "                    verbs.add(v.head)\n",
    "            \n",
    "            if not verbs:\n",
    "                continue\n",
    " \n",
    "            rootVerb = ([w for w in doc if w.head is w] or [None])[0]\n",
    "            \n",
    "            #check first the root verb and then the others.\n",
    "            verbs = [rootVerb] + list(verbs)\n",
    "            \n",
    "            for v in verbs:\n",
    "                if checkAction(v):            \n",
    "                    \n",
    "                    for np in doc.noun_chunks:\n",
    "                        if np.root.head == v:\n",
    "\n",
    "                            if(np.root.dep == nsubj):\n",
    "                                quotee = np.text\n",
    "                                quoteeFound = True\n",
    "                                \n",
    "                            if(np.root.dep == dobj): #TODO\n",
    "                                pass\n",
    "                                \n",
    "                            quoteFound = True\n",
    "                    \n",
    "                    if quoteeFound:\n",
    "                        break\n",
    "                            \n",
    "                    \n",
    "                \n",
    "            if quoteFound:\n",
    "                    quote = s                    \n",
    "                    quotee, quoteeType = resolveQuotee(quotee, doc.ents, allEntities)\n",
    "                    quotee, quoteeType = improveQuoteeEntity(quotee, quoteeType, allEntities)                    \n",
    "        \n",
    "                    quotes.append({'quote': quote, 'quotee':quotee, 'quoteeType':quoteeType})\n",
    "                    #print('quote: ', quote)\n",
    "                    #print('by: ', quotee, '(', quoteeType, ')')\n",
    "                    #print()\n",
    "                    continue\n",
    "                    \n",
    "        return quotes\n",
    "    \n",
    "    \n",
    "    #TODO add case for Spark\n",
    "    rddd = SQLContext(sc).createDataFrame(documents[['title','body']]).rdd\n",
    "    documents['quotes'] = rddd.map(lambda s: dependencyGraphSearch(s.title, s.body)).collect()\n",
    "    \n",
    "    \n",
    "    documents['quotes'] = documents.apply(lambda d: dependencyGraphSearch(d['title'],d['body']), axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    documents = documents.join(documents['quotes'].apply(pd.Series).stack().reset_index(level=1, drop=True).apply(pd.Series))\n",
    "    \n",
    "    print ('#quotesPerDocument: ',len(documents)/limitDocuments)\n",
    "    return documents\n",
    "\n",
    "documents = quoteExtraction(20)\n",
    "documents.to_pickle('quotes.pkl')\n",
    "documents[['topic_label', 'quotee']].groupby('topic_label').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
