{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import seaborn as sns\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from settings import *\n",
    "from glove import *\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from textblob import TextBlob\n",
    "from tweets_ops import *\n",
    "from url_helpers import analyze_url\n",
    "from matching import *\n",
    "from textstat.textstat import textstat\n",
    "import urllib, bs4\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import string\n",
    "\n",
    "def is_clickbait(titles):\n",
    "        cb=[]\n",
    "        os.chdir('../lib/clickbait')\n",
    "        for t in titles:\n",
    "            t = ''.join([c for c in str(t) if c in string.printable])\n",
    "            out, err = subprocess.Popen(['venv/bin/python', 'src/detect.py', t], stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n",
    "            try:\n",
    "                cb.append(float(re.findall('\\d*\\.?\\d+', str(out))[0]))\n",
    "            except:\n",
    "                cb.append(0.0)\n",
    "        os.chdir('../../src')\n",
    "        return cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlet1 = 'dailymail.co.uk'\n",
    "# outlet2 = 'nytimes.com'\n",
    "# outlet3 = 'theatlantic.com'\n",
    "# outlet4 = 'washingtonpost.com'\n",
    "\n",
    "# G = read_graph(cache_dir+'diffusion_graph/pruned_graph_v3.tsv')\n",
    "# pagerank = nx.pagerank(G.reverse())\n",
    "# betweenness_centrality = nx.betweenness_centrality(G)\n",
    "# degree_centrality = nx.degree_centrality(G)\n",
    "# in_degree_centrality = nx.in_degree_centrality(G)\n",
    "# out_degree_centrality = nx.out_degree_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet = outlet1\n",
    "df = pd.read_csv(cache_dir+'article_details_v5.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "df[df.apply(lambda x: analyze_url(x['url'])[0]==outlet, axis=1)].to_csv(cache_dir+outlet+'.tsv', sep='\\t', index=None)\n",
    "\n",
    "aggregate_tweet_details(cache_dir+'diffusion_graph/pruned_graph_v3.tsv', cache_dir+'tweet_details_v3.tsv', cache_dir+outlet+'.tsv', cache_dir+outlet+'.tsv')\n",
    "df = pd.read_csv(cache_dir+outlet+'.tsv', sep='\\t').fillna(0)\n",
    "\n",
    "for s in ['full', 'paragraph', 'sentence']:\n",
    "    train = pd.read_csv(cache_dir+'similarity_model/train_pairs_v2_'+s+'.tsv', sep='\\t').merge(df[['url']], left_on='article', right_on='url').drop('url', axis=1)\n",
    "    train = train[train.related==True]\n",
    "    test = pd.read_csv(cache_dir+'similarity_model/test_pairs_v2_'+s+'.tsv', sep='\\t').merge(df[['url']], left_on='article', right_on='url').drop('url', axis=1)\n",
    "    pd.concat([train, test]).to_csv(cache_dir+'similarity_model/exp2/'+s+'.tsv', sep='\\t', index=None)\n",
    "    \n",
    "test_similarity_model(cache_dir+'similarity_model/exp2/', cache_dir+'similarity_model/rf_model.sav', cache_dir+'similarity_model/exp2/results.tsv')\n",
    "sim = pd.read_csv(cache_dir+'similarity_model/exp2/results.tsv', sep='\\t').drop('paper', axis=1).groupby('article').max().reset_index()\n",
    "df = df.merge(sim, left_on='url', right_on='article').drop('article', axis=1)\n",
    "\n",
    "df['readability'] = df['full_text'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "df['title_subjectivity'] = df['title'].apply(lambda x: TextBlob(x).subjectivity)\n",
    "df['title_polarity'] = df['title'].apply(lambda x: TextBlob(x).polarity)\n",
    "df['title_clickbaitness'] = df['title'].apply(is_clickbait)\n",
    "\n",
    "df = pd.concat([df.drop(['quote_indicators'], axis=1), df['quote_indicators'].apply(lambda x: pd.Series(eval(x)))], axis=1)\n",
    "\n",
    "\n",
    "df['has_author'] = ~(df['authors'].isnull() | (df['authors'] == ''))\n",
    "\n",
    "df['pageRank'] = df['url'].apply(lambda x: pagerank[x])\n",
    "df['betweenness_centrality'] = df['url'].apply(lambda x: betweenness_centrality[x])\n",
    "df['degree_centrality'] = df['url'].apply(lambda x: degree_centrality[x])\n",
    "df['in_degree_centrality'] = df['url'].apply(lambda x: in_degree_centrality[x])\n",
    "df['out_degree_centrality'] = df['url'].apply(lambda x: out_degree_centrality[x])\n",
    "\n",
    "df['word_count'] = df['full_text'].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "\n",
    "df['alexa_rank']=df['url'].apply(lambda x: bs4.BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\"+str(x)).read(), \"xml\").find(\"REACH\")['RANK'])\n",
    "\n",
    "\n",
    "df.url = df.url.apply(lambda x: analyze_url(x)[0])\n",
    "\n",
    "df = df[['url', 'likes', 'replies_count', 'title_clickbaitness',\n",
    "    'betweenness_centrality', 'degree_centrality', 'in_degree_centrality', 'out_degree_centrality',\n",
    "    'replies_mean_polarity', 'replies_mean_subjectivity', 'retweets',\n",
    "    'stance', 'tweets_mean_polarity', 'tweets_mean_subjectivity',\n",
    "    'tweets_time_delta', 'users_countries', 'users_median_followers',\n",
    "    'users_median_friends', 'users_median_tweets', 'related', 'readability',\n",
    "    'title_subjectivity', 'title_polarity', 'count_all_quotes',\n",
    "    'count_PER_quotes', 'count_ORG_quotes', 'count_unnamed_quotes',\n",
    "    'has_author', 'pageRank', 'word_count', 'alexa_rank']]\n",
    "\n",
    "df.columns = ['url', '#Likes', '#Replies', 'Title Clickbaitness',\n",
    "    'Betweenness Centrality', 'Degree Centrality', 'In Degree Centrality', 'Out Degree Centrality',\n",
    "    'Replies Polarity', 'Replies Subjectivity', '#Retweets',\n",
    "    'Replies Stance', 'Tweets Polarity', 'Tweets Subjectivity',\n",
    "    'Tweets Shelf Life', '#Users Countries', '#Followers',\n",
    "    '#Users Friends', '#Users Tweets', 'STS', 'Readability',\n",
    "    'Title Subjectivity', 'Title Polarity', '#Quotes',\n",
    "    '#Person Quotes', '#Scientific Mentions', '#Weasel Quotes',\n",
    "    'Author Signature', 'Personalized PageRank', 'Article Word Count', 'Alexa Rank']\n",
    "\n",
    "df.to_csv(cache_dir+outlet+'.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cache_dir+'article_details_v5.tsv', sep='\\t').head(10)\n",
    "\n",
    "aggregate_tweet_details(cache_dir+'diffusion_graph/pruned_graph_v3.tsv', cache_dir+'tweet_details_v3.tsv', cache_dir+'article_details_v5.tsv', cache_dir+'article_details_v6.tsv')\n",
    "df = pd.read_csv(cache_dir+'article_details_v6.tsv', sep='\\t')\n",
    "\n",
    "for s in ['full', 'paragraph', 'sentence']:\n",
    "    train = pd.read_csv(cache_dir+'similarity_model/train_pairs_v2_'+s+'.tsv', sep='\\t').merge(df[['url']], left_on='article', right_on='url').drop('url', axis=1)\n",
    "    train = train[train.related==True]\n",
    "    test = pd.read_csv(cache_dir+'similarity_model/test_pairs_v2_'+s+'.tsv', sep='\\t').merge(df[['url']], left_on='article', right_on='url').drop('url', axis=1)\n",
    "    pd.concat([train, test]).to_csv(cache_dir+'similarity_model/exp2/'+s+'.tsv', sep='\\t', index=None)\n",
    "    \n",
    "test_similarity_model(cache_dir+'similarity_model/exp2/', cache_dir+'similarity_model/rf_model.sav', cache_dir+'similarity_model/exp2/results.tsv')\n",
    "sim = pd.read_csv(cache_dir+'similarity_model/exp2/results.tsv', sep='\\t').drop('paper', axis=1).fillna(0).groupby('article').max().reset_index()\n",
    "df = df.merge(sim, left_on='url', right_on='article').drop('article', axis=1)\n",
    "\n",
    "df['readability'] = df['full_text'].apply(lambda x: textstat.flesch_reading_ease(str(x)))\n",
    "df['title_subjectivity'] = df['title'].apply(lambda x: TextBlob(str(x)).subjectivity)\n",
    "df['title_polarity'] = df['title'].apply(lambda x: TextBlob(str(x)).polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_clickbaitness'] = is_clickbait(df['title'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.drop(['quote_indicators'], axis=1), df['quote_indicators'].apply(lambda x: pd.Series(eval(x)))], axis=1)\n",
    "\n",
    "\n",
    "df['has_author'] = ~(df['authors'].isnull() | (df['authors'] == ''))\n",
    "\n",
    "df['pageRank'] = df['url'].apply(lambda x: pagerank[x])\n",
    "df['betweenness_centrality'] = df['url'].apply(lambda x: betweenness_centrality[x])\n",
    "df['degree_centrality'] = df['url'].apply(lambda x: degree_centrality[x])\n",
    "df['in_degree_centrality'] = df['url'].apply(lambda x: in_degree_centrality[x])\n",
    "df['out_degree_centrality'] = df['url'].apply(lambda x: out_degree_centrality[x])\n",
    "\n",
    "df['word_count'] = df['full_text'].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "\n",
    "df['alexa_rank']=df['url'].apply(lambda x: bs4.BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\"+str(x)).read(), \"xml\").find(\"REACH\")['RANK'])\n",
    "\n",
    "\n",
    "df.url = df.url.apply(lambda x: analyze_url(x)[0])\n",
    "\n",
    "df = df[['url', 'likes', 'replies_count', 'title_clickbaitness',\n",
    "    'betweenness_centrality', 'degree_centrality', 'in_degree_centrality', 'out_degree_centrality',\n",
    "    'replies_mean_polarity', 'replies_mean_subjectivity', 'retweets',\n",
    "    'stance', 'tweets_mean_polarity', 'tweets_mean_subjectivity',\n",
    "    'tweets_time_delta', 'users_countries', 'users_median_followers',\n",
    "    'users_median_friends', 'users_median_tweets', 'related', 'readability',\n",
    "    'title_subjectivity', 'title_polarity', 'count_all_quotes',\n",
    "    'count_PER_quotes', 'count_ORG_quotes', 'count_unnamed_quotes',\n",
    "    'has_author', 'pageRank', 'word_count', 'alexa_rank']]\n",
    "\n",
    "df.columns = ['url', '#Likes', '#Replies', 'Title Clickbaitness',\n",
    "    'Betweenness Centrality', 'Degree Centrality', 'In Degree Centrality', 'Out Degree Centrality',\n",
    "    'Replies Polarity', 'Replies Subjectivity', '#Retweets',\n",
    "    'Replies Stance', 'Tweets Polarity', 'Tweets Subjectivity',\n",
    "    'Tweets Shelf Life', '#Users Countries', '#Followers',\n",
    "    '#Users Friends', '#Users Tweets', 'STS', 'Readability',\n",
    "    'Title Subjectivity', 'Title Polarity', '#Quotes',\n",
    "    '#Person Quotes', '#Scientific Mentions', '#Weasel Quotes',\n",
    "    'Author Signature', 'Personalized PageRank', 'Article Word Count', 'Alexa Rank']\n",
    "\n",
    "df.to_csv(cache_dir+'article_details_v6.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(cache_dir+outlet2+'.tsv', sep='\\t'), pd.read_csv(cache_dir+outlet3+'.tsv', sep='\\t')])\n",
    "y = df[['url']].values\n",
    "X = np.array(df.drop('url', axis=1).values, dtype=np.float32)\n",
    "X = (X - X.mean(axis=0)) / (X.std(axis=0)+1e-9)\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "_, result = f_classif(X,y)\n",
    "\n",
    "d = {c:r for c, r in zip(df.columns[1:], result)}\n",
    "df = pd.DataFrame(sorted(d.items(), key=lambda kv: kv[1]))\n",
    "\n",
    "df[0] = df.apply(lambda x: x[0]+'**' if float(x[1])<.005 else x[0]+'*' if float(x[1])<.01 else x[0], axis=1)\n",
    "\n",
    "df23 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df12, df13, df23, df123], axis=1).head(15)[0]\n",
    "df.columns = ['NYTimes-DailyMail', 'TheAtlantic-DailyMail', 'NYTimes-TheAtlantic', 'NYTimes-TheAtlantic-DailyMail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
