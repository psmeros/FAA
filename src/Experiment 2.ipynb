{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import seaborn as sns\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from settings import *\n",
    "from glove import *\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from textblob import TextBlob\n",
    "from tweets_ops import *\n",
    "from url_helpers import analyze_url\n",
    "from matching import *\n",
    "from textstat.textstat import textstat\n",
    "import urllib, bs4\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import string\n",
    "\n",
    "def is_clickbait(title):\n",
    "    \n",
    "    title = ''.join([c for c in title if c in string.printable])\n",
    "    os.chdir('../lib/clickbait')\n",
    "    out, err = subprocess.Popen(['venv/bin/python', 'src/detect.py', title], stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n",
    "    os.chdir('../../src')\n",
    "    return float(re.findall('\\d*\\.?\\d+', str(out))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet1 = 'dailymail.co.uk'\n",
    "outlet2 = 'nytimes.com'\n",
    "outlet3 = 'theatlantic.com'\n",
    "outlet4 = 'washingtonpost.com'\n",
    "#DailyMail -> Bad\n",
    "#NYTimes -> Neutral for acsh Top for climatefeedback\n",
    "#the Atlantic -> Top for acsh Neutral for climatefeedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet = outlet3\n",
    "df = pd.read_csv(cache_dir+'article_details_v5.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "df[df.apply(lambda x: analyze_url(x['url'])[0]==outlet, axis=1)].to_csv(cache_dir+outlet+'.tsv', sep='\\t', index=None)\n",
    "\n",
    "aggregate_tweet_details(cache_dir+'diffusion_graph/pruned_graph_v3.tsv', cache_dir+'tweet_details_v3.tsv', cache_dir+outlet+'.tsv', cache_dir+outlet+'.tsv')\n",
    "df = pd.read_csv(cache_dir+outlet+'.tsv', sep='\\t').fillna(0)\n",
    "\n",
    "for s in ['full', 'paragraph', 'sentence']:\n",
    "    train = pd.read_csv(cache_dir+'similarity_model/train_pairs_v2_'+s+'.tsv', sep='\\t').merge(df[['url']], left_on='article', right_on='url').drop('url', axis=1)\n",
    "    train = train[train.related==True]\n",
    "    test = pd.read_csv(cache_dir+'similarity_model/test_pairs_v2_'+s+'.tsv', sep='\\t').merge(df[['url']], left_on='article', right_on='url').drop('url', axis=1)\n",
    "    pd.concat([train, test]).to_csv(cache_dir+'similarity_model/exp2/'+s+'.tsv', sep='\\t', index=None)\n",
    "    \n",
    "test_similarity_model(cache_dir+'similarity_model/exp2/', cache_dir+'similarity_model/rf_model.sav', cache_dir+'similarity_model/exp2/results.tsv')\n",
    "sim = pd.read_csv(cache_dir+'similarity_model/exp2/results.tsv', sep='\\t').drop('paper', axis=1).groupby('article').max().reset_index()\n",
    "df = df.merge(sim, left_on='url', right_on='article').drop('article', axis=1)\n",
    "\n",
    "df['readability'] = df['full_text'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "df['title_subjectivity'] = df['title'].apply(lambda x: TextBlob(x).subjectivity)\n",
    "df['title_polarity'] = df['title'].apply(lambda x: TextBlob(x).polarity)\n",
    "df['title_clickbaitness'] = df['title'].apply(is_clickbait)\n",
    "\n",
    "df = pd.concat([df.drop(['quote_indicators'], axis=1), df['quote_indicators'].apply(lambda x: pd.Series(eval(x)))], axis=1)\n",
    "\n",
    "\n",
    "df['has_author'] = ~(df['authors'].isnull() | (df['authors'] == ''))\n",
    "\n",
    "G = read_graph(cache_dir+'diffusion_graph/pruned_graph_v3.tsv')\n",
    "pagerank = nx.pagerank(G.reverse())\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "in_degree_centrality = nx.in_degree_centrality(G)\n",
    "out_degree_centrality = nx.out_degree_centrality(G)\n",
    "\n",
    "df['pageRank'] = df['url'].apply(lambda x: pagerank[x])\n",
    "df['betweenness_centrality'] = df['url'].apply(lambda x: betweenness_centrality[x])\n",
    "df['degree_centrality'] = df['url'].apply(lambda x: degree_centrality[x])\n",
    "df['in_degree_centrality'] = df['url'].apply(lambda x: in_degree_centrality[x])\n",
    "df['out_degree_centrality'] = df['url'].apply(lambda x: out_degree_centrality[x])\n",
    "\n",
    "df['word_count'] = df['full_text'].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "\n",
    "df['alexa_rank']=df['url'].apply(lambda x: bs4.BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\"+str(x)).read(), \"xml\").find(\"REACH\")['RANK'])\n",
    "\n",
    "\n",
    "df.url = df.url.apply(lambda x: analyze_url(x)[0])\n",
    "\n",
    "df[['url', 'likes', 'replies_count', 'title_clickbaitness',\n",
    "    'betweenness_centrality', 'degree_centrality', 'in_degree_centrality', 'out_degree_centrality',\n",
    "    'replies_mean_polarity', 'replies_mean_subjectivity', 'retweets',\n",
    "    'stance', 'tweets_mean_polarity', 'tweets_mean_subjectivity',\n",
    "    'tweets_time_delta', 'users_countries', 'users_median_followers',\n",
    "    'users_median_friends', 'users_median_tweets', 'related', 'readability',\n",
    "    'title_subjectivity', 'title_polarity', 'count_all_quotes',\n",
    "    'count_PER_quotes', 'count_ORG_quotes', 'count_unnamed_quotes',\n",
    "    'has_author', 'pageRank', 'word_count', 'alexa_rank']].to_csv(cache_dir+outlet+'.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('word_count', 0.028109597),\n",
       " ('replies_count', 0.044994574),\n",
       " ('users_median_followers', 0.23002414),\n",
       " ('count_ORG_quotes', 0.2667348),\n",
       " ('replies_mean_subjectivity', 0.26783),\n",
       " ('replies_mean_polarity', 0.32936358),\n",
       " ('pageRank', 0.35574126),\n",
       " ('tweets_time_delta', 0.3587253),\n",
       " ('users_countries', 0.36107644),\n",
       " ('title_polarity', 0.4024514),\n",
       " ('retweets', 0.44539905),\n",
       " ('betweenness_centrality', 0.4777195),\n",
       " ('users_median_friends', 0.4793642),\n",
       " ('degree_centrality', 0.49266532),\n",
       " ('in_degree_centrality', 0.49426708),\n",
       " ('tweets_mean_subjectivity', 0.5460902),\n",
       " ('related', 0.60506415),\n",
       " ('count_all_quotes', 0.74475163),\n",
       " ('tweets_mean_polarity', 0.7536425),\n",
       " ('count_unnamed_quotes', 0.75515056),\n",
       " ('readability', 0.7747717),\n",
       " ('count_PER_quotes', 0.8162522),\n",
       " ('title_subjectivity', 0.84063345),\n",
       " ('stance', 0.8988162),\n",
       " ('title_clickbaitness', 0.936268),\n",
       " ('likes', 0.95009863),\n",
       " ('users_median_tweets', 0.9668238),\n",
       " ('out_degree_centrality', 0.99999994),\n",
       " ('has_author', nan),\n",
       " ('alexa_rank', nan)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv(cache_dir+outlet2+'.tsv', sep='\\t'), pd.read_csv(cache_dir+outlet3+'.tsv', sep='\\t')])\n",
    "y = df[['url']].values\n",
    "X = np.array(df.drop('url', axis=1).values, dtype=np.float32)\n",
    "X = (X - X.mean(axis=0)) / (X.std(axis=0)+1e-9)\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "_, result = f_classif(X,y)\n",
    "\n",
    "d = {c:r for c, r in zip(df.columns[1:], result)}\n",
    "sorted(d.items(), key=lambda kv: kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
