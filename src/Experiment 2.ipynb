{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import seaborn as sns\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from settings import *\n",
    "from glove import *\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from textblob import TextBlob\n",
    "from tweets_ops import *\n",
    "from url_helpers import analyze_url\n",
    "from matching import *\n",
    "from textstat.textstat import textstat\n",
    "import urllib, bs4\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "title = 'Novak Djokovic stunned as Australian Open title defence ends against Denis Istomin'\n",
    "os.chdir('../lib/clickbait')\n",
    "a,b = subprocess.Popen(['venv/bin/python', 'src/detect.py', title], stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n",
    "os.chdir('../../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'headline is 0.85 % clickbaity\\n', b'Using TensorFlow backend.\\n')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet1 = 'dailymail.co.uk'\n",
    "outlet2 = 'nytimes.com'\n",
    "outlet3 = 'theatlantic.com'\n",
    "outlet4 = 'washingtonpost.com'\n",
    "#DailyMail -> Bad\n",
    "#NYTimes -> Neutral for acsh Top for climatefeedback\n",
    "#the Atlantic -> Top for acsh Neutral for climatefeedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet = outlet3\n",
    "df = pd.read_csv(cache_dir+'article_details_v5.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "df[df.apply(lambda x: analyze_url(x['url'])[0]==outlet, axis=1)].to_csv(cache_dir+outlet+'.tsv', sep='\\t', index=None)\n",
    "\n",
    "aggregate_tweet_details(cache_dir+'diffusion_graph/pruned_graph_v3.tsv', cache_dir+'tweet_details_v3.tsv', cache_dir+outlet+'.tsv', cache_dir+outlet+'.tsv')\n",
    "df = pd.read_csv(cache_dir+outlet+'.tsv', sep='\\t').fillna(0)\n",
    "\n",
    "for s in ['full', 'paragraph', 'sentence']:\n",
    "    train = pd.read_csv(cache_dir+'similarity_model/train_pairs_v2_'+s+'.tsv', sep='\\t').merge(df[['url']], left_on='article', right_on='url').drop('url', axis=1)\n",
    "    train = train[train.related==True]\n",
    "    test = pd.read_csv(cache_dir+'similarity_model/test_pairs_v2_'+s+'.tsv', sep='\\t').merge(df[['url']], left_on='article', right_on='url').drop('url', axis=1)\n",
    "    pd.concat([train, test]).to_csv(cache_dir+'similarity_model/exp2/'+s+'.tsv', sep='\\t', index=None)\n",
    "    \n",
    "test_similarity_model(cache_dir+'similarity_model/exp2/', cache_dir+'similarity_model/rf_model.sav', cache_dir+'similarity_model/exp2/results.tsv')\n",
    "sim = pd.read_csv(cache_dir+'similarity_model/exp2/results.tsv', sep='\\t').drop('paper', axis=1).groupby('article').max().reset_index()\n",
    "df = df.merge(sim, left_on='url', right_on='article').drop('article', axis=1)\n",
    "\n",
    "df['readability'] = df['full_text'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "df['title_subjectivity'] = df['title'].apply(lambda x: TextBlob(x).subjectivity)\n",
    "df['title_polarity'] = df['title'].apply(lambda x: TextBlob(x).polarity)\n",
    "\n",
    "df = pd.concat([df.drop(['quote_indicators'], axis=1), df['quote_indicators'].apply(lambda x: pd.Series(eval(x)))], axis=1)\n",
    "\n",
    "\n",
    "df['has_author'] = ~(df['authors'].isnull() | (df['authors'] == ''))\n",
    "\n",
    "G = read_graph(cache_dir+'diffusion_graph/pruned_graph_v3.tsv')\n",
    "pr = nx.pagerank(G.reverse())\n",
    "\n",
    "df['pageRank'] = df['url'].apply(lambda x: pr[x])\n",
    "\n",
    "df['word_count'] = df['full_text'].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "\n",
    "df['alexa_rank']=df['url'].apply(lambda x: bs4.BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\"+str(x)).read(), \"xml\").find(\"REACH\")['RANK'])\n",
    "\n",
    "\n",
    "df.url = df.url.apply(lambda x: analyze_url(x)[0])\n",
    "\n",
    "df[['url', 'likes', 'replies_count',\n",
    "    'replies_mean_polarity', 'replies_mean_subjectivity', 'retweets',\n",
    "    'stance', 'tweets_mean_polarity', 'tweets_mean_subjectivity',\n",
    "    'tweets_time_delta', 'users_countries', 'users_median_followers',\n",
    "    'users_median_friends', 'users_median_tweets', 'related', 'readability',\n",
    "    'title_subjectivity', 'title_polarity', 'count_all_quotes',\n",
    "    'count_PER_quotes', 'count_ORG_quotes', 'count_unnamed_quotes',\n",
    "    'has_author', 'pageRank', 'word_count', 'alexa_rank']].to_csv(cache_dir+outlet+'.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.read_csv(cache_dir+outlet1+'.tsv', sep='\\t'), pd.read_csv(cache_dir+outlet2+'.tsv', sep='\\t'), pd.read_csv(cache_dir+outlet3+'.tsv', sep='\\t')])\n",
    "y = df[['url']].values\n",
    "X = np.array(df.drop('url', axis=1).values, dtype=np.float32)\n",
    "X = (X - X.mean(axis=0)) / (X.std(axis=0)+1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_est = 100\n",
    "# m_dep = 10\n",
    "# classifier = RandomForestClassifier(n_estimators=n_est, max_depth=m_dep, n_jobs=-1)\n",
    "# classifier.fit(X, y)\n",
    "# result = classifier.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "_, result = f_classif(X,y)\n",
    "\n",
    "d = {c:r for c, r in zip(df.columns[1:], result)}\n",
    "sorted(d.items(), key=lambda kv: kv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
