{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import seaborn as sns\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from settings import *\n",
    "from glove import *\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from textblob import TextBlob\n",
    "from tweets_ops import *\n",
    "from url_helpers import analyze_url\n",
    "from matching import *\n",
    "from textstat.textstat import textstat\n",
    "import urllib, bs4\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import string\n",
    "\n",
    "def is_clickbait(title):\n",
    "    \n",
    "    title = ''.join([c for c in title if c in string.printable])\n",
    "    os.chdir('../lib/clickbait')\n",
    "    out, err = subprocess.Popen(['venv/bin/python', 'src/detect.py', title], stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n",
    "    os.chdir('../../src')\n",
    "    return float(re.findall('\\d*\\.?\\d+', str(out))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet1 = 'dailymail.co.uk'\n",
    "outlet2 = 'nytimes.com'\n",
    "outlet3 = 'theatlantic.com'\n",
    "outlet4 = 'washingtonpost.com'\n",
    "\n",
    "G = read_graph(cache_dir+'diffusion_graph/pruned_graph_v3.tsv')\n",
    "pagerank = nx.pagerank(G.reverse())\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "in_degree_centrality = nx.in_degree_centrality(G)\n",
    "out_degree_centrality = nx.out_degree_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet = outlet3\n",
    "df = pd.read_csv(cache_dir+'article_details_v5.tsv', sep='\\t')\n",
    "\n",
    "\n",
    "df[df.apply(lambda x: analyze_url(x['url'])[0]==outlet, axis=1)].to_csv(cache_dir+outlet+'.tsv', sep='\\t', index=None)\n",
    "\n",
    "aggregate_tweet_details(cache_dir+'diffusion_graph/pruned_graph_v3.tsv', cache_dir+'tweet_details_v3.tsv', cache_dir+outlet+'.tsv', cache_dir+outlet+'.tsv')\n",
    "df = pd.read_csv(cache_dir+outlet+'.tsv', sep='\\t').fillna(0)\n",
    "\n",
    "for s in ['full', 'paragraph', 'sentence']:\n",
    "    train = pd.read_csv(cache_dir+'similarity_model/train_pairs_v2_'+s+'.tsv', sep='\\t').merge(df[['url']], left_on='article', right_on='url').drop('url', axis=1)\n",
    "    train = train[train.related==True]\n",
    "    test = pd.read_csv(cache_dir+'similarity_model/test_pairs_v2_'+s+'.tsv', sep='\\t').merge(df[['url']], left_on='article', right_on='url').drop('url', axis=1)\n",
    "    pd.concat([train, test]).to_csv(cache_dir+'similarity_model/exp2/'+s+'.tsv', sep='\\t', index=None)\n",
    "    \n",
    "test_similarity_model(cache_dir+'similarity_model/exp2/', cache_dir+'similarity_model/rf_model.sav', cache_dir+'similarity_model/exp2/results.tsv')\n",
    "sim = pd.read_csv(cache_dir+'similarity_model/exp2/results.tsv', sep='\\t').drop('paper', axis=1).groupby('article').max().reset_index()\n",
    "df = df.merge(sim, left_on='url', right_on='article').drop('article', axis=1)\n",
    "\n",
    "df['readability'] = df['full_text'].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "df['title_subjectivity'] = df['title'].apply(lambda x: TextBlob(x).subjectivity)\n",
    "df['title_polarity'] = df['title'].apply(lambda x: TextBlob(x).polarity)\n",
    "df['title_clickbaitness'] = df['title'].apply(is_clickbait)\n",
    "\n",
    "df = pd.concat([df.drop(['quote_indicators'], axis=1), df['quote_indicators'].apply(lambda x: pd.Series(eval(x)))], axis=1)\n",
    "\n",
    "\n",
    "df['has_author'] = ~(df['authors'].isnull() | (df['authors'] == ''))\n",
    "\n",
    "df['pageRank'] = df['url'].apply(lambda x: pagerank[x])\n",
    "df['betweenness_centrality'] = df['url'].apply(lambda x: betweenness_centrality[x])\n",
    "df['degree_centrality'] = df['url'].apply(lambda x: degree_centrality[x])\n",
    "df['in_degree_centrality'] = df['url'].apply(lambda x: in_degree_centrality[x])\n",
    "df['out_degree_centrality'] = df['url'].apply(lambda x: out_degree_centrality[x])\n",
    "\n",
    "df['word_count'] = df['full_text'].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "\n",
    "df['alexa_rank']=df['url'].apply(lambda x: bs4.BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\"+str(x)).read(), \"xml\").find(\"REACH\")['RANK'])\n",
    "\n",
    "\n",
    "df.url = df.url.apply(lambda x: analyze_url(x)[0])\n",
    "\n",
    "df = df[['url', 'likes', 'replies_count', 'title_clickbaitness',\n",
    "    'betweenness_centrality', 'degree_centrality', 'in_degree_centrality', 'out_degree_centrality',\n",
    "    'replies_mean_polarity', 'replies_mean_subjectivity', 'retweets',\n",
    "    'stance', 'tweets_mean_polarity', 'tweets_mean_subjectivity',\n",
    "    'tweets_time_delta', 'users_countries', 'users_median_followers',\n",
    "    'users_median_friends', 'users_median_tweets', 'related', 'readability',\n",
    "    'title_subjectivity', 'title_polarity', 'count_all_quotes',\n",
    "    'count_PER_quotes', 'count_ORG_quotes', 'count_unnamed_quotes',\n",
    "    'has_author', 'pageRank', 'word_count', 'alexa_rank']]\n",
    "\n",
    "df.columns = ['url', '#Likes', '#Replies', 'Title Clickbaitness',\n",
    "    'Betweenness Centrality', 'Degree Centrality', 'In Degree Centrality', 'Out Degree Centrality',\n",
    "    'Replies Polarity', 'Replies Subjectivity', '#Retweets',\n",
    "    'Replies Stance', 'Tweets Polarity', 'Tweets Subjectivity',\n",
    "    'Tweets Shelf Life', '#Users Countries', '#Followers',\n",
    "    '#Users Friends', '#Users Tweets', 'STS', 'Readability',\n",
    "    'Title Subjectivity', 'Title Polarity', '#Quotes',\n",
    "    '#Person Quotes', '#Scientific Mentions', '#Weasel Quotes',\n",
    "    'Author Signature', 'Personalized PageRank', 'Article Word Count', 'Alexa Rank']\n",
    "\n",
    "df.to_csv(cache_dir+outlet+'.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(cache_dir+'article_details_v5.tsv', sep='\\t')\n",
    "\n",
    "aggregate_tweet_details(cache_dir+'diffusion_graph/pruned_graph_v3.tsv', cache_dir+'tweet_details_v3.tsv', cache_dir+'article_details_v5.tsv', cache_dir+'article_details_v6.tsv')\n",
    "df = pd.read_csv(cache_dir+'article_details_v6.tsv', sep='\\t').fillna(0)\n",
    "\n",
    "for s in ['full', 'paragraph', 'sentence']:\n",
    "    train = pd.read_csv(cache_dir+'similarity_model/train_pairs_v2_'+s+'.tsv', sep='\\t').merge(df[['url']], left_on='article', right_on='url').drop('url', axis=1)\n",
    "    train = train[train.related==True]\n",
    "    test = pd.read_csv(cache_dir+'similarity_model/test_pairs_v2_'+s+'.tsv', sep='\\t').merge(df[['url']], left_on='article', right_on='url').drop('url', axis=1)\n",
    "    pd.concat([train, test]).to_csv(cache_dir+'similarity_model/exp2/'+s+'.tsv', sep='\\t', index=None)\n",
    "    \n",
    "test_similarity_model(cache_dir+'similarity_model/exp2/', cache_dir+'similarity_model/rf_model.sav', cache_dir+'similarity_model/exp2/results.tsv')\n",
    "sim = pd.read_csv(cache_dir+'similarity_model/exp2/results.tsv', sep='\\t').drop('paper', axis=1).groupby('article').max().reset_index()\n",
    "df = df.merge(sim, left_on='url', right_on='article').drop('article', axis=1)\n",
    "\n",
    "df['readability'] = df['full_text'].apply(lambda x: textstat.flesch_reading_ease(str(x)))\n",
    "df['title_subjectivity'] = df['title'].apply(lambda x: TextBlob(str(x)).subjectivity)\n",
    "df['title_polarity'] = df['title'].apply(lambda x: TextBlob(str(x)).polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7aee7fc28f42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title_clickbaitness'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_clickbait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-302385108c63>\u001b[0m in \u001b[0;36mis_clickbait\u001b[0;34m(title)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_clickbait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../lib/clickbait'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'venv/bin/python'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'src/detect.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "df['title_clickbaitness'] = df['title'].apply(is_clickbait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df.drop(['quote_indicators'], axis=1), df['quote_indicators'].apply(lambda x: pd.Series(eval(x)))], axis=1)\n",
    "\n",
    "\n",
    "df['has_author'] = ~(df['authors'].isnull() | (df['authors'] == ''))\n",
    "\n",
    "df['pageRank'] = df['url'].apply(lambda x: pagerank[x])\n",
    "df['betweenness_centrality'] = df['url'].apply(lambda x: betweenness_centrality[x])\n",
    "df['degree_centrality'] = df['url'].apply(lambda x: degree_centrality[x])\n",
    "df['in_degree_centrality'] = df['url'].apply(lambda x: in_degree_centrality[x])\n",
    "df['out_degree_centrality'] = df['url'].apply(lambda x: out_degree_centrality[x])\n",
    "\n",
    "df['word_count'] = df['full_text'].apply(lambda x: len(re.findall(r'\\w+', x)))\n",
    "\n",
    "df['alexa_rank']=df['url'].apply(lambda x: bs4.BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\"+str(x)).read(), \"xml\").find(\"REACH\")['RANK'])\n",
    "\n",
    "\n",
    "df.url = df.url.apply(lambda x: analyze_url(x)[0])\n",
    "\n",
    "df = df[['url', 'likes', 'replies_count', 'title_clickbaitness',\n",
    "    'betweenness_centrality', 'degree_centrality', 'in_degree_centrality', 'out_degree_centrality',\n",
    "    'replies_mean_polarity', 'replies_mean_subjectivity', 'retweets',\n",
    "    'stance', 'tweets_mean_polarity', 'tweets_mean_subjectivity',\n",
    "    'tweets_time_delta', 'users_countries', 'users_median_followers',\n",
    "    'users_median_friends', 'users_median_tweets', 'related', 'readability',\n",
    "    'title_subjectivity', 'title_polarity', 'count_all_quotes',\n",
    "    'count_PER_quotes', 'count_ORG_quotes', 'count_unnamed_quotes',\n",
    "    'has_author', 'pageRank', 'word_count', 'alexa_rank']]\n",
    "\n",
    "df.columns = ['url', '#Likes', '#Replies', 'Title Clickbaitness',\n",
    "    'Betweenness Centrality', 'Degree Centrality', 'In Degree Centrality', 'Out Degree Centrality',\n",
    "    'Replies Polarity', 'Replies Subjectivity', '#Retweets',\n",
    "    'Replies Stance', 'Tweets Polarity', 'Tweets Subjectivity',\n",
    "    'Tweets Shelf Life', '#Users Countries', '#Followers',\n",
    "    '#Users Friends', '#Users Tweets', 'STS', 'Readability',\n",
    "    'Title Subjectivity', 'Title Polarity', '#Quotes',\n",
    "    '#Person Quotes', '#Scientific Mentions', '#Weasel Quotes',\n",
    "    'Author Signature', 'Personalized PageRank', 'Article Word Count', 'Alexa Rank']\n",
    "\n",
    "df.to_csv(cache_dir+'article_details_v6.tsv', sep='\\t', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/sklearn/feature_selection/univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([pd.read_csv(cache_dir+outlet2+'.tsv', sep='\\t'), pd.read_csv(cache_dir+outlet3+'.tsv', sep='\\t')])\n",
    "y = df[['url']].values\n",
    "X = np.array(df.drop('url', axis=1).values, dtype=np.float32)\n",
    "X = (X - X.mean(axis=0)) / (X.std(axis=0)+1e-9)\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "_, result = f_classif(X,y)\n",
    "\n",
    "d = {c:r for c, r in zip(df.columns[1:], result)}\n",
    "df = pd.DataFrame(sorted(d.items(), key=lambda kv: kv[1]))\n",
    "\n",
    "df[0] = df.apply(lambda x: x[0]+'**' if float(x[1])<.005 else x[0]+'*' if float(x[1])<.01 else x[0], axis=1)\n",
    "\n",
    "df23 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df12, df13, df23, df123], axis=1).head(15)[0]\n",
    "df.columns = ['NYTimes-DailyMail', 'TheAtlantic-DailyMail', 'NYTimes-TheAtlantic', 'NYTimes-TheAtlantic-DailyMail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llll}\n",
      "\\toprule\n",
      "      NYTimes-DailyMail &   TheAtlantic-DailyMail &     NYTimes-TheAtlantic & NYTimes-TheAtlantic-DailyMail \\\\\n",
      "\\midrule\n",
      "           Alexa Rank** &            Alexa Rank** &      Article Word Count &        \\#Scientific Mentions** \\\\\n",
      " \\#Scientific Mentions** &  \\#Scientific Mentions** &                \\#Replies &           Article Word Count* \\\\\n",
      "               \\#Quotes* &                 \\#Quotes &              \\#Followers &                       \\#Quotes \\\\\n",
      "    Article Word Count* &          Title Polarity &    \\#Scientific Mentions &                Title Polarity \\\\\n",
      "         Title Polarity &          \\#Users Friends &    Replies Subjectivity &                      \\#Replies \\\\\n",
      "        Tweets Polarity &    Replies Subjectivity &        Replies Polarity &                \\#Weasel Quotes \\\\\n",
      "         \\#Weasel Quotes &       Tweets Shelf Life &   Personalized PageRank &               Tweets Polarity \\\\\n",
      "    Tweets Subjectivity &         Tweets Polarity &       Tweets Shelf Life &          Replies Subjectivity \\\\\n",
      "    Title Clickbaitness &      Article Word Count &        \\#Users Countries &           Tweets Subjectivity \\\\\n",
      "   Replies Subjectivity &     Title Clickbaitness &          Title Polarity &                    \\#Followers \\\\\n",
      "  Out Degree Centrality &          \\#Weasel Quotes &               \\#Retweets &           Title Clickbaitness \\\\\n",
      "         Replies Stance &   Out Degree Centrality &  Betweenness Centrality &             Tweets Shelf Life \\\\\n",
      " Betweenness Centrality &        Replies Polarity &          \\#Users Friends &              \\#Users Countries \\\\\n",
      "     Title Subjectivity &  Betweenness Centrality &       Degree Centrality &              Replies Polarity \\\\\n",
      "      Degree Centrality &              \\#Followers &    In Degree Centrality &         Personalized PageRank \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
